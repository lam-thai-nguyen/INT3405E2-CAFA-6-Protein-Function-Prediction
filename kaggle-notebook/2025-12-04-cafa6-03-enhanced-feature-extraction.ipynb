{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAFA6-03-Enhanced feature extraction\n",
    "\n",
    "Improvement from CAFA6-02: 77 hand-crafted features --> 100 automatic features.\n",
    "\n",
    "Using: Meta-AI ESM-2 + PCA\n",
    "\n",
    "ESM-2 is a protein language model (PLM) created by Meta AI. It is trained on sequences of amino acid, just like what we are having as inputs, and then output a vector embedding.\n",
    "\n",
    "Future direction: Use MLP instead of OneVsRestClassifier(LogisticRegression)\n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n",
    "- (ESM-2 embeddings 320 features) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n",
    "- (ProtT5 embeddings 1024 features) https://www.kaggle.com/code/ahsuna123/t5-embedding-calculation-cafa-6/output?select=train_ids.npy\n",
    "- (MLP with ESM2) https://www.kaggle.com/code/jwang2025learning/cafa-6-function-prediction-using-prott5?scriptVersionId=282801093\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with ESM-2 (esm2_t6_8M_UR50D)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:53:37.801083Z",
     "iopub.status.busy": "2025-12-04T13:53:37.800185Z",
     "iopub.status.idle": "2025-12-04T13:53:43.160038Z",
     "shell.execute_reply": "2025-12-04T13:53:43.158892Z",
     "shell.execute_reply.started": "2025-12-04T13:53:37.801041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to C:\\Users\\MS/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to C:\\Users\\MS/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D-contact-regression.pt\n",
      "torch.Size([1, 320])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load lightest ESM-2\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "model.eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Example sequence\n",
    "sequences = [(\"protein1\", \"MKTAYIAKQRQISFVKSHFSRQDILD\")]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[6], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "# Mean-pool over sequence tokens\n",
    "sequence_embedding = token_representations[:, 1:-1].mean(1)\n",
    "\n",
    "print(sequence_embedding.shape)  # (1, 320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load CAFA6 files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:55:23.735352Z",
     "iopub.status.busy": "2025-12-04T13:55:23.734987Z",
     "iopub.status.idle": "2025-12-04T13:55:24.169646Z",
     "shell.execute_reply": "2025-12-04T13:55:24.168123Z",
     "shell.execute_reply.started": "2025-12-04T13:55:23.735325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO  # parse fasta file\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:55:26.928707Z",
     "iopub.status.busy": "2025-12-04T13:55:26.927450Z",
     "iopub.status.idle": "2025-12-04T13:55:26.934052Z",
     "shell.execute_reply": "2025-12-04T13:55:26.932765Z",
     "shell.execute_reply.started": "2025-12-04T13:55:26.928648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CAFA6 file paths\n",
    "TRAIN_TERMS = \"data/Train/train_terms.tsv\"\n",
    "TRAIN_SEQ = \"data/Train/train_sequences.fasta\"\n",
    "TRAIN_TAXONOMY = \"data/Train/train_taxonomy.tsv\"\n",
    "TEST_SEQ = \"data/Test/testsuperset.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:12:11.508755Z",
     "iopub.status.busy": "2025-12-04T14:12:11.507739Z",
     "iopub.status.idle": "2025-12-04T14:12:12.424743Z",
     "shell.execute_reply": "2025-12-04T14:12:12.423452Z",
     "shell.execute_reply.started": "2025-12-04T14:12:11.508711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load CAFA6 files into dataFrame\n",
    "train_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")  # identifier --> label\n",
    "train_taxonomy_df = pd.read_csv(TRAIN_TAXONOMY, sep=\"\\t\", names=['EntryID', 'taxonomyID'])  # identifier --> taxonomy\n",
    "\n",
    "def load_fasta_to_dataframe(file_path, is_train=True):\n",
    "    records = []\n",
    "    parser = SeqIO.parse(file_path, \"fasta\")\n",
    "    for record in parser:\n",
    "        entry_id = record.id.split('|')[1] if is_train and '|' in record.id else record.id.split()[0]\n",
    "        records.append({'EntryID': entry_id, 'sequence': str(record.seq)})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "train_sequences_df = load_fasta_to_dataframe(TRAIN_SEQ, is_train=True)  # identifier --> input: amino acid sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:12:23.853160Z",
     "iopub.status.busy": "2025-12-04T14:12:23.852798Z",
     "iopub.status.idle": "2025-12-04T14:12:25.822161Z",
     "shell.execute_reply": "2025-12-04T14:12:25.821096Z",
     "shell.execute_reply.started": "2025-12-04T14:12:23.853128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group the dataFrames above into a single data frame\n",
    "protein_labels = train_terms_df.groupby('EntryID')['term'].apply(list).reset_index(name='labels')  # turn all EntryID duplicates into one EntryID with their terms forming a list\n",
    "train_df_eda = pd.merge(train_sequences_df, train_taxonomy_df, on='EntryID', how='left')\n",
    "train_df_eda = pd.merge(train_df_eda, protein_labels, on='EntryID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:12:31.818083Z",
     "iopub.status.busy": "2025-12-04T14:12:31.816669Z",
     "iopub.status.idle": "2025-12-04T14:12:31.845608Z",
     "shell.execute_reply": "2025-12-04T14:12:31.844662Z",
     "shell.execute_reply.started": "2025-12-04T14:12:31.818038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>taxonomyID</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0C5B5G6</td>\n",
       "      <td>MRWQEMGYIFYPRKLR</td>\n",
       "      <td>9606</td>\n",
       "      <td>[GO:0001649, GO:0033687, GO:0005615, GO:000563...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0JNW5</td>\n",
       "      <td>MAGIIKKQILKHLSRFTKNLSPDKINLSTLKGEGELKNLELDEEVL...</td>\n",
       "      <td>9606</td>\n",
       "      <td>[GO:0120013, GO:0034498, GO:0005769, GO:012000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JP26</td>\n",
       "      <td>MVAEVCSMPAASAVKKPFDLRSKMGKWCHHRFPCCRGSGKSNMGTS...</td>\n",
       "      <td>9606</td>\n",
       "      <td>[GO:0005515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0PK11</td>\n",
       "      <td>MPGWFKKAWYGLASLLSFSSFILIIVALVVPHWLSGKILCQTGVDL...</td>\n",
       "      <td>9606</td>\n",
       "      <td>[GO:0007605, GO:0005515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1A4S6</td>\n",
       "      <td>MGLQPLEFSDCYLDSPWFRERIRAHEAELERTNKFIKELIKDGKNL...</td>\n",
       "      <td>9606</td>\n",
       "      <td>[GO:0005829, GO:0010008, GO:0005515, GO:000509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82399</th>\n",
       "      <td>Q9UTM1</td>\n",
       "      <td>MSKLKAQSALQKLIESQKNPNANEDGYFRRKRLAKKERPFEPKKLV...</td>\n",
       "      <td>284812</td>\n",
       "      <td>[GO:0005730]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82400</th>\n",
       "      <td>Q9Y7I1</td>\n",
       "      <td>MSSNSNTDHSTGDNRSKSEKQTDLRNALRETESHGMPPLRGPAGFP...</td>\n",
       "      <td>284812</td>\n",
       "      <td>[GO:0005634, GO:0005829]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82401</th>\n",
       "      <td>Q9Y7P7</td>\n",
       "      <td>MRSNNSSLVHCCWVSPPSLTRLPAFPSPRILSPCYCYNKRIRPFRG...</td>\n",
       "      <td>284812</td>\n",
       "      <td>[GO:0005634, GO:0005829]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82402</th>\n",
       "      <td>Q9Y7Q3</td>\n",
       "      <td>MHSSRRKYNDMWTARLLIRSDQKEEKYPSFKKNAGKAINAHLIPKL...</td>\n",
       "      <td>284812</td>\n",
       "      <td>[GO:0005634, GO:0005739, GO:0005829]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82403</th>\n",
       "      <td>Q9Y816</td>\n",
       "      <td>MITEFIKSFLLFFFLPFFLSMPMIFATLGEFTDDQTHHYSTLPSCD...</td>\n",
       "      <td>284812</td>\n",
       "      <td>[GO:0005737, GO:0005783]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82404 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EntryID                                           sequence  \\\n",
       "0      A0A0C5B5G6                                   MRWQEMGYIFYPRKLR   \n",
       "1          A0JNW5  MAGIIKKQILKHLSRFTKNLSPDKINLSTLKGEGELKNLELDEEVL...   \n",
       "2          A0JP26  MVAEVCSMPAASAVKKPFDLRSKMGKWCHHRFPCCRGSGKSNMGTS...   \n",
       "3          A0PK11  MPGWFKKAWYGLASLLSFSSFILIIVALVVPHWLSGKILCQTGVDL...   \n",
       "4          A1A4S6  MGLQPLEFSDCYLDSPWFRERIRAHEAELERTNKFIKELIKDGKNL...   \n",
       "...           ...                                                ...   \n",
       "82399      Q9UTM1  MSKLKAQSALQKLIESQKNPNANEDGYFRRKRLAKKERPFEPKKLV...   \n",
       "82400      Q9Y7I1  MSSNSNTDHSTGDNRSKSEKQTDLRNALRETESHGMPPLRGPAGFP...   \n",
       "82401      Q9Y7P7  MRSNNSSLVHCCWVSPPSLTRLPAFPSPRILSPCYCYNKRIRPFRG...   \n",
       "82402      Q9Y7Q3  MHSSRRKYNDMWTARLLIRSDQKEEKYPSFKKNAGKAINAHLIPKL...   \n",
       "82403      Q9Y816  MITEFIKSFLLFFFLPFFLSMPMIFATLGEFTDDQTHHYSTLPSCD...   \n",
       "\n",
       "       taxonomyID                                             labels  \n",
       "0            9606  [GO:0001649, GO:0033687, GO:0005615, GO:000563...  \n",
       "1            9606  [GO:0120013, GO:0034498, GO:0005769, GO:012000...  \n",
       "2            9606                                       [GO:0005515]  \n",
       "3            9606                           [GO:0007605, GO:0005515]  \n",
       "4            9606  [GO:0005829, GO:0010008, GO:0005515, GO:000509...  \n",
       "...           ...                                                ...  \n",
       "82399      284812                                       [GO:0005730]  \n",
       "82400      284812                           [GO:0005634, GO:0005829]  \n",
       "82401      284812                           [GO:0005634, GO:0005829]  \n",
       "82402      284812               [GO:0005634, GO:0005739, GO:0005829]  \n",
       "82403      284812                           [GO:0005737, GO:0005783]  \n",
       "\n",
       "[82404 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:55:27.772171Z",
     "iopub.status.busy": "2025-12-04T13:55:27.771814Z",
     "iopub.status.idle": "2025-12-04T13:55:30.809070Z",
     "shell.execute_reply": "2025-12-04T13:55:30.807898Z",
     "shell.execute_reply.started": "2025-12-04T13:55:27.772145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82404 train and 224309 test sequences\n"
     ]
    }
   ],
   "source": [
    "# Dict {entryId, seq}\n",
    "train_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\n",
    "test_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n",
    "\n",
    "print(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T13:57:42.364984Z",
     "iopub.status.busy": "2025-12-04T13:57:42.364634Z",
     "iopub.status.idle": "2025-12-04T13:57:42.428063Z",
     "shell.execute_reply": "2025-12-04T13:57:42.427101Z",
     "shell.execute_reply.started": "2025-12-04T13:57:42.364962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\n",
      "Test dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dict:\", list(train_sequences.items())[0])\n",
    "print(\"Test dict:\", list(test_sequences.items())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature extraction\n",
    "\n",
    "We use PCA + ESM-2 to squash 320 features into 100 features.\n",
    "\n",
    "WHY?\n",
    "\n",
    "OneVsRestClassifier(LogisticRegression) predicts each GO term with a linear model. This, combined with 320 features from ESM-2, makes the training time too long. In short, it is inefficient in design.\n",
    "\n",
    "Future direction: Use MLP\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Embeddings file paths\n",
    "ESM_EMBEDDINGS = \"ESM2/\"\n",
    "TRAIN_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_train.npy\"\n",
    "TRAIN_IDS = ESM_EMBEDDINGS + \"/train_ids.npy\"\n",
    "TEST_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_test.npy\"\n",
    "TEST_IDS = ESM_EMBEDDINGS + \"/test_ids.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:02:19.983426Z",
     "iopub.status.busy": "2025-12-04T14:02:19.983069Z",
     "iopub.status.idle": "2025-12-04T14:02:21.864845Z",
     "shell.execute_reply": "2025-12-04T14:02:21.863766Z",
     "shell.execute_reply.started": "2025-12-04T14:02:19.983401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "X_train = np.load(TRAIN_EMBEDDINGS)\n",
    "train_ids = np.load(TRAIN_IDS)\n",
    "X_test = np.load(TEST_EMBEDDINGS)\n",
    "test_ids = np.load(TEST_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:02:21.866567Z",
     "iopub.status.busy": "2025-12-04T14:02:21.866218Z",
     "iopub.status.idle": "2025-12-04T14:02:21.872599Z",
     "shell.execute_reply": "2025-12-04T14:02:21.871605Z",
     "shell.execute_reply.started": "2025-12-04T14:02:21.866537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (82404, 320)\n",
      "train_ids shape: (82404,)\n",
      "X_test shape: (224309, 320)\n",
      "test_ids shape: (224309,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"train_ids shape:\", train_ids.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"test_ids shape:\", test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:09:12.681591Z",
     "iopub.status.busy": "2025-12-04T14:09:12.681200Z",
     "iopub.status.idle": "2025-12-04T14:09:16.434972Z",
     "shell.execute_reply": "2025-12-04T14:09:16.433894Z",
     "shell.execute_reply.started": "2025-12-04T14:09:12.681563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:09:33.951974Z",
     "iopub.status.busy": "2025-12-04T14:09:33.951641Z",
     "iopub.status.idle": "2025-12-04T14:09:33.957805Z",
     "shell.execute_reply": "2025-12-04T14:09:33.956595Z",
     "shell.execute_reply.started": "2025-12-04T14:09:33.951947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reduced shape: (82404, 100)\n",
      "X_test_reduced shape: (224309, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_reduced shape:\", X_train_reduced.shape)\n",
    "print(\"X_test_reduced shape:\", X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ As illustrated above, the features `X_train` are of shape (82404, 77) and the `protein_identifies` stores 82404 entryIDs accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Label encoding and Training\n",
    "\n",
    "We divide all labels into three subsets (one for MF, one for BP, and one for CC).\n",
    "-  As a result, for each sequence, we will have 3 vectors as follows using multi-hot encoding (i.e. simply one-hot for multi-classification problems)\n",
    "    - For MF: [0 1 0 1 ... 0] of length num_unique(MF_GO_temrs)\n",
    "    - For BP: [0 1 0 1 ... 0] of length num_unique(BP_GO_temrs)\n",
    "    - For CC: [0 1 0 1 ... 0] of length num_unique(CC_GO_temrs)\n",
    "- Then we will train three separate models for each ontology. We use three models to predict for a single example in the test set, and gather the predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:29:04.738585Z",
     "iopub.status.busy": "2025-12-04T14:29:04.738234Z",
     "iopub.status.idle": "2025-12-04T14:29:07.543409Z",
     "shell.execute_reply": "2025-12-04T14:29:07.542088Z",
     "shell.execute_reply.started": "2025-12-04T14:29:04.738557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape for P ontology: (82404, 16858) \t\t Number of unique P terms: 16858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [35:04<1:10:08, 2104.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for P trained successfully.\n",
      "y_train shape for C ontology: (82404, 2651) \t\t Number of unique C terms: 2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [40:35<17:41, 1061.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for C trained successfully.\n",
      "y_train shape for F ontology: (82404, 6616) \t\t Number of unique F terms: 6616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [54:07<00:00, 1082.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for F trained successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create three label sets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "mlb_dict = dict()\n",
    "models = dict()\n",
    "\n",
    "entry_ids = [s.split('|')[1] for s in train_ids]  # 82404 entryIDs in the orginal order  \n",
    "\n",
    "for aspect in tqdm(['P', 'C', 'F'], desc=\"Training models\"):\n",
    "    # Filter the train_terms_df based on aspect\n",
    "    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n",
    "\n",
    "    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n",
    "    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n",
    "    # This ensures y_train is of shape (82404, ...)\n",
    "    labels = [protein_terms.get(entry_id, []) for entry_id in entry_ids]\n",
    "\n",
    "    # Multi-hot encoding, use sparse representation\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    y_train = mlb.fit_transform(labels)\n",
    "    \n",
    "    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t\\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n",
    "\n",
    "    # Save to dict\n",
    "    mlb_dict[aspect] = mlb\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=600, solver='lbfgs', C=0.5, random_state=42), n_jobs=-1)\n",
    "    model.fit(X_train_reduced, y_train)\n",
    "    models[aspect] = model\n",
    "    print(f\"Model for {aspect} trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:29:24.134336Z",
     "iopub.status.busy": "2025-12-04T14:29:24.133964Z",
     "iopub.status.idle": "2025-12-04T14:29:24.145455Z",
     "shell.execute_reply": "2025-12-04T14:29:24.144321Z",
     "shell.execute_reply.started": "2025-12-04T14:29:24.134311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Test Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [41:42<00:00, 55.60s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000  # avoid memory overflow\n",
    "submission_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting on Test Set\"):\n",
    "    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n",
    "\n",
    "    # Slice PCA-reduced features\n",
    "    X_batch = X_test_reduced[i : i + BATCH_SIZE]\n",
    "\n",
    "    for aspect, model in models.items():\n",
    "        mlb = mlb_dict[aspect]\n",
    "        y_pred_proba = model.predict_proba(X_batch)\n",
    "\n",
    "        for j, entry_id in enumerate(batch_entry_ids):\n",
    "            probs = y_pred_proba[j]\n",
    "            candidate_indices = np.where(probs > 0.02)[0]\n",
    "\n",
    "            for idx in candidate_indices:\n",
    "                submission_list.append((entry_id, mlb.classes_[idx], round(probs[idx], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:29:29.833508Z",
     "iopub.status.busy": "2025-12-04T14:29:29.832571Z",
     "iopub.status.idle": "2025-12-04T14:29:29.856356Z",
     "shell.execute_reply": "2025-12-04T14:29:29.855453Z",
     "shell.execute_reply.started": "2025-12-04T14:29:29.833474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying 1500 prediction limit per protein...\n",
      "\n",
      "Submission file 'submission.tsv' created successfully.\n",
      "Total predictions in final submission: 5,214,598\n",
      "Submission DataFrame Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein Id</th>\n",
       "      <th>GO Term Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005886</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005515</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005829</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005739</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005789</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protein Id  GO Term Id  Prediction\n",
       "0  A0A017SE81  GO:0005886       0.151\n",
       "1  A0A017SE81  GO:0005515       0.135\n",
       "2  A0A017SE81  GO:0005829       0.096\n",
       "3  A0A017SE81  GO:0005739       0.079\n",
       "4  A0A017SE81  GO:0005789       0.076"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(submission_list, columns=['Protein Id', 'GO Term Id', 'Prediction'])\n",
    "\n",
    "print(\"Applying 1500 prediction limit per protein...\")\n",
    "submission_df = submission_df.sort_values(by=['Protein Id', 'Prediction'], ascending=[True, False])\n",
    "final_submission_df = submission_df.groupby('Protein Id').head(1500).reset_index(drop=True)\n",
    "final_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.tsv' created successfully.\")\n",
    "print(f\"Total predictions in final submission: {len(final_submission_df):,}\")\n",
    "print(\"Submission DataFrame Head:\")\n",
    "display(final_submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8917191,
     "sourceId": 13991224,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
