{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13991224,"sourceType":"datasetVersion","datasetId":8917191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### CAFA6-07-Model Improvement\n\nImprovement from CAFA6-03: OneVsRest(LogisticRegression) --> Customized MLP\n\nFuture direction: Use model ensemble\n\nReferences:\n- https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n- (ESM-2 embeddings 320 features) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n- (MLP with ESM2) https://www.kaggle.com/code/jwang2025learning/cafa-6-function-prediction-using-prott5?scriptVersionId=282801093\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install biopython > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:50.099049Z","iopub.execute_input":"2025-12-05T07:38:50.099515Z","iopub.status.idle":"2025-12-05T07:38:55.773928Z","shell.execute_reply.started":"2025-12-05T07:38:50.099489Z","shell.execute_reply":"2025-12-05T07:38:55.773110Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Load CAFA6 files\n\n---","metadata":{}},{"cell_type":"code","source":"from Bio import SeqIO  # parse fasta file\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:55.775582Z","iopub.execute_input":"2025-12-05T07:38:55.775952Z","iopub.status.idle":"2025-12-05T07:38:56.090646Z","shell.execute_reply.started":"2025-12-05T07:38:55.775927Z","shell.execute_reply":"2025-12-05T07:38:56.090103Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# CAFA6 file paths\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTRAIN_TAXONOMY = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\nTEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:56.091294Z","iopub.execute_input":"2025-12-05T07:38:56.091616Z","iopub.status.idle":"2025-12-05T07:38:56.095260Z","shell.execute_reply.started":"2025-12-05T07:38:56.091600Z","shell.execute_reply":"2025-12-05T07:38:56.094587Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dict {entryId, seq}\ntrain_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\ntest_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:56.097136Z","iopub.execute_input":"2025-12-05T07:38:56.097418Z","iopub.status.idle":"2025-12-05T07:38:59.121415Z","shell.execute_reply.started":"2025-12-05T07:38:56.097397Z","shell.execute_reply":"2025-12-05T07:38:59.120704Z"}},"outputs":[{"name":"stdout","text":"Loaded 82404 train and 224309 test sequences\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Train dict:\", list(train_sequences.items())[0])\nprint(\"Test dict:\", list(test_sequences.items())[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:59.122218Z","iopub.execute_input":"2025-12-05T07:38:59.122457Z","iopub.status.idle":"2025-12-05T07:38:59.246896Z","shell.execute_reply.started":"2025-12-05T07:38:59.122434Z","shell.execute_reply":"2025-12-05T07:38:59.246121Z"}},"outputs":[{"name":"stdout","text":"Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\nTest dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_ids = [i.split('|')[1] for i in train_sequences.keys()]\ntest_ids = list(test_sequences.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:59.247687Z","iopub.execute_input":"2025-12-05T07:38:59.247965Z","iopub.status.idle":"2025-12-05T07:38:59.277226Z","shell.execute_reply.started":"2025-12-05T07:38:59.247935Z","shell.execute_reply":"2025-12-05T07:38:59.276564Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 2: Feature extraction\n\nWe use the 20 features from the baseline\n  \n---","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nstandard_aas = 'ACDEFGHIKLMNPQRSTVWY'  # 20 standard axit amin sequence characters\n\ndef extract_sequence_features(seq):\n    \"\"\"Convert a protein sequence into a 20-dim AAC vector.\"\"\"\n    length = len(seq)\n    counts = Counter(seq)\n    return np.array([counts.get(aa, 0) / length for aa in standard_aas])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:59.277961Z","iopub.execute_input":"2025-12-05T07:38:59.278202Z","iopub.status.idle":"2025-12-05T07:38:59.282249Z","shell.execute_reply.started":"2025-12-05T07:38:59.278183Z","shell.execute_reply":"2025-12-05T07:38:59.281663Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"X_train = np.array([extract_sequence_features(i) for i in train_ids])\nprint(\"X_train shape:\", X_train.shape)\nX_test = np.array([extract_sequence_features(i) for i in test_ids])\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:38:59.282965Z","iopub.execute_input":"2025-12-05T07:38:59.283238Z","iopub.status.idle":"2025-12-05T07:39:01.080052Z","shell.execute_reply.started":"2025-12-05T07:38:59.283220Z","shell.execute_reply":"2025-12-05T07:39:01.079426Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (82404, 20)\nX_test shape: (224309, 20)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Normalization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_norm = scaler.fit_transform(X_train)\nX_test_norm = scaler.transform(X_test)\nprint(\"X_train_norm shape:\", X_train_norm.shape)\nprint(\"X_test_norm shape:\", X_test_norm.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:01.080708Z","iopub.execute_input":"2025-12-05T07:39:01.080959Z","iopub.status.idle":"2025-12-05T07:39:01.580420Z","shell.execute_reply.started":"2025-12-05T07:39:01.080932Z","shell.execute_reply":"2025-12-05T07:39:01.579656Z"}},"outputs":[{"name":"stdout","text":"X_train_norm shape: (82404, 20)\nX_test_norm shape: (224309, 20)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Step 3: Customized MLP\n\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:29.690906Z","iopub.execute_input":"2025-12-05T07:39:29.691589Z","iopub.status.idle":"2025-12-05T07:39:29.695798Z","shell.execute_reply.started":"2025-12-05T07:39:29.691563Z","shell.execute_reply":"2025-12-05T07:39:29.694993Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class MLPClassifier(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.35),\n\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.3),\n\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(32, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:31.295185Z","iopub.execute_input":"2025-12-05T07:39:31.295769Z","iopub.status.idle":"2025-12-05T07:39:31.300742Z","shell.execute_reply.started":"2025-12-05T07:39:31.295746Z","shell.execute_reply":"2025-12-05T07:39:31.300016Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Step 4: Label encoding\n\nWe divide all labels into three subsets (one for MF, one for BP, and one for CC).\n-  As a result, for each sequence, we will have 3 vectors as follows using multi-hot encoding (i.e. simply one-hot for multi-classification problems)\n    - For MF: [0 1 0 1 ... 0] of length num_unique(MF_GO_temrs)\n    - For BP: [0 1 0 1 ... 0] of length num_unique(BP_GO_temrs)\n    - For CC: [0 1 0 1 ... 0] of length num_unique(CC_GO_temrs)\n- Then we will train three separate models for each ontology. We use three models to predict for a single example in the test set, and gather the predictions.\n\n---","metadata":{}},{"cell_type":"code","source":"# Create three label sets\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\n\ny_trains = dict()\nmlb_dict = dict()\nmodels = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    # Filter the train_terms_df based on aspect\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n\n    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n\n    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n    # This ensures y_train is of shape (82404, ...)\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    # Multi-hot encoding, use sparse representation\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    y_trains[aspect] = y_train\n    \n    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t\\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n\n    # Save to dict\n    mlb_dict[aspect] = mlb\n    model = MLPClassifier(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n    models[aspect] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:34.259025Z","iopub.execute_input":"2025-12-05T07:39:34.259726Z","iopub.status.idle":"2025-12-05T07:39:37.895981Z","shell.execute_reply.started":"2025-12-05T07:39:34.259700Z","shell.execute_reply":"2025-12-05T07:39:37.895314Z"}},"outputs":[{"name":"stdout","text":"y_train shape for P ontology: (82404, 16858) \t\t Number of unique P terms: 16858\ny_train shape for C ontology: (82404, 2651) \t\t Number of unique C terms: 2651\ny_train shape for F ontology: (82404, 6616) \t\t Number of unique F terms: 6616\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:37.897256Z","iopub.execute_input":"2025-12-05T07:39:37.897512Z","iopub.status.idle":"2025-12-05T07:39:37.903260Z","shell.execute_reply.started":"2025-12-05T07:39:37.897494Z","shell.execute_reply":"2025-12-05T07:39:37.902608Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'P': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=20, out_features=256, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.35, inplace=False)\n     (4): Linear(in_features=256, out_features=128, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.3, inplace=False)\n     (8): Linear(in_features=128, out_features=64, bias=True)\n     (9): ReLU()\n     (10): Dropout(p=0.25, inplace=False)\n     (11): Linear(in_features=64, out_features=32, bias=True)\n     (12): ReLU()\n     (13): Dropout(p=0.2, inplace=False)\n     (14): Linear(in_features=32, out_features=16858, bias=True)\n   )\n ),\n 'C': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=20, out_features=256, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.35, inplace=False)\n     (4): Linear(in_features=256, out_features=128, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.3, inplace=False)\n     (8): Linear(in_features=128, out_features=64, bias=True)\n     (9): ReLU()\n     (10): Dropout(p=0.25, inplace=False)\n     (11): Linear(in_features=64, out_features=32, bias=True)\n     (12): ReLU()\n     (13): Dropout(p=0.2, inplace=False)\n     (14): Linear(in_features=32, out_features=2651, bias=True)\n   )\n ),\n 'F': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=20, out_features=256, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.35, inplace=False)\n     (4): Linear(in_features=256, out_features=128, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.3, inplace=False)\n     (8): Linear(in_features=128, out_features=64, bias=True)\n     (9): ReLU()\n     (10): Dropout(p=0.25, inplace=False)\n     (11): Linear(in_features=64, out_features=32, bias=True)\n     (12): ReLU()\n     (13): Dropout(p=0.2, inplace=False)\n     (14): Linear(in_features=32, out_features=6616, bias=True)\n   )\n )}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"y_trains","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:37.904057Z","iopub.execute_input":"2025-12-05T07:39:37.904282Z","iopub.status.idle":"2025-12-05T07:39:37.918695Z","shell.execute_reply.started":"2025-12-05T07:39:37.904267Z","shell.execute_reply":"2025-12-05T07:39:37.918175Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 250805 stored elements and shape (82404, 16858)>,\n 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 157770 stored elements and shape (82404, 2651)>,\n 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 128452 stored elements and shape (82404, 6616)>}"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Step 5: Model training","metadata":{}},{"cell_type":"code","source":"# DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n\nloaders = {}\n\nfor aspect in ['P', 'C', 'F']:\n    # Convert sparse CSR → dense numpy → float tensor\n    y_dense = y_trains[aspect].toarray().astype('float32')\n    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n\n    dataset = TensorDataset(X_train_tensor, y_tensor)\n\n    loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n\n    print(aspect, \"loader ready:\", y_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:38.286067Z","iopub.execute_input":"2025-12-05T07:39:38.286388Z","iopub.status.idle":"2025-12-05T07:39:49.929317Z","shell.execute_reply.started":"2025-12-05T07:39:38.286360Z","shell.execute_reply":"2025-12-05T07:39:49.928561Z"}},"outputs":[{"name":"stdout","text":"P loader ready: torch.Size([82404, 16858])\nC loader ready: torch.Size([82404, 2651])\nF loader ready: torch.Size([82404, 6616])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Training\nimport torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\n\ndef train_one_model(model, loader, epochs=5, lr=1e-3, device='cuda'):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for ep in range(epochs):\n        total_loss = 0.0\n\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            logits = model(X_batch)               # shape: (batch, num_labels)\n            loss = criterion(logits, y_batch)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {ep+1}/{epochs}   Loss = {total_loss/len(loader):.4f}\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:49.930481Z","iopub.execute_input":"2025-12-05T07:39:49.930871Z","iopub.status.idle":"2025-12-05T07:39:49.937344Z","shell.execute_reply.started":"2025-12-05T07:39:49.930829Z","shell.execute_reply":"2025-12-05T07:39:49.936700Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Train 3 models\ntrained_models = dict()\n\nfor aspect in ['P', 'C', 'F']:\n    print(\"\\nTraining\", aspect, \"...\")\n    trained_models[aspect] = train_one_model(\n        model=models[aspect],\n        loader=loaders[aspect],\n        epochs=8,\n        lr=1e-3,\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:39:49.938025Z","iopub.execute_input":"2025-12-05T07:39:49.938276Z","iopub.status.idle":"2025-12-05T07:41:14.024008Z","shell.execute_reply.started":"2025-12-05T07:39:49.938254Z","shell.execute_reply":"2025-12-05T07:41:14.023220Z"}},"outputs":[{"name":"stdout","text":"\nTraining P ...\nEpoch 1/8   Loss = 0.0342\nEpoch 2/8   Loss = 0.0020\nEpoch 3/8   Loss = 0.0019\nEpoch 4/8   Loss = 0.0018\nEpoch 5/8   Loss = 0.0018\nEpoch 6/8   Loss = 0.0017\nEpoch 7/8   Loss = 0.0017\nEpoch 8/8   Loss = 0.0017\n\nTraining C ...\nEpoch 1/8   Loss = 0.0360\nEpoch 2/8   Loss = 0.0046\nEpoch 3/8   Loss = 0.0044\nEpoch 4/8   Loss = 0.0043\nEpoch 5/8   Loss = 0.0043\nEpoch 6/8   Loss = 0.0043\nEpoch 7/8   Loss = 0.0042\nEpoch 8/8   Loss = 0.0042\n\nTraining F ...\nEpoch 1/8   Loss = 0.0332\nEpoch 2/8   Loss = 0.0019\nEpoch 3/8   Loss = 0.0018\nEpoch 4/8   Loss = 0.0017\nEpoch 5/8   Loss = 0.0017\nEpoch 6/8   Loss = 0.0017\nEpoch 7/8   Loss = 0.0017\nEpoch 8/8   Loss = 0.0016\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 5: Inference and Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 5000  # avoid memory overflow\nsubmission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting on Test Set\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    # Slice features\n    X_batch = X_test_norm[i : i + BATCH_SIZE]\n    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n\n    # For each ontology aspect (P, F, C)\n    for aspect, model in trained_models.items():\n        model.eval()\n\n        # Forward pass (logits → probabilities)\n        with torch.no_grad():\n            logits = model(X_batch)\n            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray (batch, num_labels)\n\n        mlb = mlb_dict[aspect]  # MultiLabelBinarizer for this aspect\n\n        # Loop over proteins in the batch\n        for j, entry_id in enumerate(batch_entry_ids):\n            prob_vec = probs[j]\n\n            # threshold at 0.02\n            candidate_indices = np.where(prob_vec > 0.02)[0]\n\n            for idx in candidate_indices:\n                submission_list.append(\n                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:41:14.025736Z","iopub.execute_input":"2025-12-05T07:41:14.026199Z","iopub.status.idle":"2025-12-05T07:42:10.387664Z","shell.execute_reply.started":"2025-12-05T07:41:14.026179Z","shell.execute_reply":"2025-12-05T07:42:10.387084Z"}},"outputs":[{"name":"stderr","text":"Predicting on Test Set: 100%|██████████| 45/45 [00:56<00:00,  1.25s/it]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"submission_df = pd.DataFrame(\n    submission_list,\n    columns=['Protein Id', 'GO Term Id', 'Prediction']\n)\n\nsubmission_df = submission_df.sort_values(\n    by=['Protein Id', 'Prediction'],\n    ascending=[True, False]\n)\n\n# Limit 1500 predictions per protein\nfinal_submission_df = (\n    submission_df.groupby('Protein Id')\n    .head(1500)\n    .reset_index(drop=True)\n)\n\nfinal_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:42:10.388362Z","iopub.execute_input":"2025-12-05T07:42:10.388604Z","iopub.status.idle":"2025-12-05T07:42:18.628251Z","shell.execute_reply.started":"2025-12-05T07:42:10.388585Z","shell.execute_reply":"2025-12-05T07:42:18.627607Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(\"\\nSubmission file 'submission.tsv' created successfully.\")\nprint(f\"Total predictions in final submission: {len(final_submission_df):,}\")\nprint(\"Submission DataFrame Head:\")\ndisplay(final_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T07:42:18.629014Z","iopub.execute_input":"2025-12-05T07:42:18.629240Z","iopub.status.idle":"2025-12-05T07:42:18.646932Z","shell.execute_reply.started":"2025-12-05T07:42:18.629215Z","shell.execute_reply":"2025-12-05T07:42:18.646357Z"}},"outputs":[{"name":"stdout","text":"\nSubmission file 'submission.tsv' created successfully.\nTotal predictions in final submission: 4,033,311\nSubmission DataFrame Head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Protein Id  GO Term Id  Prediction\n0  A0A017SE81  GO:0005515       0.422\n1  A0A017SE81  GO:0005829       0.157\n2  A0A017SE81  GO:0005634       0.153\n3  A0A017SE81  GO:0005886       0.125\n4  A0A017SE81  GO:0005737       0.105","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Protein Id</th>\n      <th>GO Term Id</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005515</td>\n      <td>0.422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005829</td>\n      <td>0.157</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005634</td>\n      <td>0.153</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005886</td>\n      <td>0.125</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005737</td>\n      <td>0.105</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22}]}