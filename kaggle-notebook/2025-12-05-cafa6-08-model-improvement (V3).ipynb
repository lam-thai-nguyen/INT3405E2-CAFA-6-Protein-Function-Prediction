{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAFA6-08-Model Improvement\n",
    "\n",
    "Improvement from CAFA6-07: 20 features to 320 features\n",
    "\n",
    "Future direction: Use model ensemble\n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n",
    "- (ESM-2 embeddings 320 features) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n",
    "- (MLP with ESM2) https://www.kaggle.com/code/jwang2025learning/cafa-6-function-prediction-using-prott5?scriptVersionId=282801093\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:02.914412Z",
     "iopub.status.busy": "2025-12-05T08:22:02.913746Z",
     "iopub.status.idle": "2025-12-05T08:22:06.336964Z",
     "shell.execute_reply": "2025-12-05T08:22:06.336108Z",
     "shell.execute_reply.started": "2025-12-05T08:22:02.914381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install biopython > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load CAFA6 files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:06.338961Z",
     "iopub.status.busy": "2025-12-05T08:22:06.338694Z",
     "iopub.status.idle": "2025-12-05T08:22:06.343213Z",
     "shell.execute_reply": "2025-12-05T08:22:06.342509Z",
     "shell.execute_reply.started": "2025-12-05T08:22:06.338927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO  # parse fasta file\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:06.344388Z",
     "iopub.status.busy": "2025-12-05T08:22:06.344130Z",
     "iopub.status.idle": "2025-12-05T08:22:06.359277Z",
     "shell.execute_reply": "2025-12-05T08:22:06.358584Z",
     "shell.execute_reply.started": "2025-12-05T08:22:06.344367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CAFA6 file paths\n",
    "TRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n",
    "TRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\n",
    "TRAIN_TAXONOMY = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\n",
    "TEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:06.360230Z",
     "iopub.status.busy": "2025-12-05T08:22:06.359982Z",
     "iopub.status.idle": "2025-12-05T08:22:07.798924Z",
     "shell.execute_reply": "2025-12-05T08:22:07.798303Z",
     "shell.execute_reply.started": "2025-12-05T08:22:06.360207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 82404 train and 224309 test sequences\n"
     ]
    }
   ],
   "source": [
    "# Dict {entryId, seq}\n",
    "train_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\n",
    "test_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n",
    "\n",
    "print(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:07.801491Z",
     "iopub.status.busy": "2025-12-05T08:22:07.801233Z",
     "iopub.status.idle": "2025-12-05T08:22:07.856848Z",
     "shell.execute_reply": "2025-12-05T08:22:07.856155Z",
     "shell.execute_reply.started": "2025-12-05T08:22:07.801472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\n",
      "Test dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dict:\", list(train_sequences.items())[0])\n",
    "print(\"Test dict:\", list(test_sequences.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:07.857916Z",
     "iopub.status.busy": "2025-12-05T08:22:07.857582Z",
     "iopub.status.idle": "2025-12-05T08:22:07.893362Z",
     "shell.execute_reply": "2025-12-05T08:22:07.892603Z",
     "shell.execute_reply.started": "2025-12-05T08:22:07.857892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ids = [i.split('|')[1] for i in train_sequences.keys()]\n",
    "test_ids = list(test_sequences.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature extraction\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:07.894507Z",
     "iopub.status.busy": "2025-12-05T08:22:07.894236Z",
     "iopub.status.idle": "2025-12-05T08:22:07.897925Z",
     "shell.execute_reply": "2025-12-05T08:22:07.897220Z",
     "shell.execute_reply.started": "2025-12-05T08:22:07.894485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Embeddings file paths\n",
    "ESM_EMBEDDINGS = \"/kaggle/input/esm2-t6-8m-ur50d-cafa6\"\n",
    "TRAIN_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_train.npy\"\n",
    "TEST_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_test.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:07.898759Z",
     "iopub.status.busy": "2025-12-05T08:22:07.898546Z",
     "iopub.status.idle": "2025-12-05T08:22:08.035046Z",
     "shell.execute_reply": "2025-12-05T08:22:08.034466Z",
     "shell.execute_reply.started": "2025-12-05T08:22:07.898738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "X_train = np.load(TRAIN_EMBEDDINGS)\n",
    "X_test = np.load(TEST_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:08.035992Z",
     "iopub.status.busy": "2025-12-05T08:22:08.035785Z",
     "iopub.status.idle": "2025-12-05T08:22:08.040067Z",
     "shell.execute_reply": "2025-12-05T08:22:08.039456Z",
     "shell.execute_reply.started": "2025-12-05T08:22:08.035977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (82404, 320)\n",
      "X_test shape: (224309, 320)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:08.041161Z",
     "iopub.status.busy": "2025-12-05T08:22:08.040929Z",
     "iopub.status.idle": "2025-12-05T08:22:08.948560Z",
     "shell.execute_reply": "2025-12-05T08:22:08.947804Z",
     "shell.execute_reply.started": "2025-12-05T08:22:08.041146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm shape: (82404, 320)\n",
      "X_test_norm shape: (224309, 320)\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "print(\"X_train_norm shape:\", X_train_norm.shape)\n",
    "print(\"X_test_norm shape:\", X_test_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Customized MLP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:08.949914Z",
     "iopub.status.busy": "2025-12-05T08:22:08.949378Z",
     "iopub.status.idle": "2025-12-05T08:22:08.954497Z",
     "shell.execute_reply": "2025-12-05T08:22:08.953797Z",
     "shell.execute_reply.started": "2025-12-05T08:22:08.949884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:08.955557Z",
     "iopub.status.busy": "2025-12-05T08:22:08.955295Z",
     "iopub.status.idle": "2025-12-05T08:22:08.972719Z",
     "shell.execute_reply": "2025-12-05T08:22:08.972007Z",
     "shell.execute_reply.started": "2025-12-05T08:22:08.955533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(1024, output_dim)  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Label encoding\n",
    "\n",
    "We divide all labels into three subsets (one for MF, one for BP, and one for CC).\n",
    "-  As a result, for each sequence, we will have 3 vectors as follows using multi-hot encoding (i.e. simply one-hot for multi-classification problems)\n",
    "    - For MF: [0 1 0 1 ... 0] of length num_unique(MF_GO_temrs)\n",
    "    - For BP: [0 1 0 1 ... 0] of length num_unique(BP_GO_temrs)\n",
    "    - For CC: [0 1 0 1 ... 0] of length num_unique(CC_GO_temrs)\n",
    "- Then we will train three separate models for each ontology. We use three models to predict for a single example in the test set, and gather the predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:08.973658Z",
     "iopub.status.busy": "2025-12-05T08:22:08.973408Z",
     "iopub.status.idle": "2025-12-05T08:22:13.078073Z",
     "shell.execute_reply": "2025-12-05T08:22:13.077462Z",
     "shell.execute_reply.started": "2025-12-05T08:22:08.973641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape for P ontology: (82404, 16858) \t\t Number of unique P terms: 16858\n",
      "y_train shape for C ontology: (82404, 2651) \t\t Number of unique C terms: 2651\n",
      "y_train shape for F ontology: (82404, 6616) \t\t Number of unique F terms: 6616\n"
     ]
    }
   ],
   "source": [
    "# Create three label sets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_trains = dict()\n",
    "mlb_dict = dict()\n",
    "models = dict()\n",
    "\n",
    "train_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n",
    "\n",
    "for aspect in ['P', 'C', 'F']:\n",
    "    # Filter the train_terms_df based on aspect\n",
    "    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n",
    "\n",
    "    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n",
    "    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n",
    "    # This ensures y_train is of shape (82404, ...)\n",
    "    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n",
    "\n",
    "    # Multi-hot encoding, use sparse representation\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    y_train = mlb.fit_transform(labels)\n",
    "    y_trains[aspect] = y_train\n",
    "    \n",
    "    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t\\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n",
    "\n",
    "    # Save to dict\n",
    "    mlb_dict[aspect] = mlb\n",
    "    model = MLPClassifier(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "    models[aspect] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:13.080476Z",
     "iopub.status.busy": "2025-12-05T08:22:13.080227Z",
     "iopub.status.idle": "2025-12-05T08:22:13.085452Z",
     "shell.execute_reply": "2025-12-05T08:22:13.084873Z",
     "shell.execute_reply.started": "2025-12-05T08:22:13.080458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': MLPClassifier(\n",
       "   (net): Sequential(\n",
       "     (0): Linear(in_features=320, out_features=2048, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): Dropout(p=0.3, inplace=False)\n",
       "     (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (7): Dropout(p=0.25, inplace=False)\n",
       "     (8): Linear(in_features=1024, out_features=16858, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'C': MLPClassifier(\n",
       "   (net): Sequential(\n",
       "     (0): Linear(in_features=320, out_features=2048, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): Dropout(p=0.3, inplace=False)\n",
       "     (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (7): Dropout(p=0.25, inplace=False)\n",
       "     (8): Linear(in_features=1024, out_features=2651, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'F': MLPClassifier(\n",
       "   (net): Sequential(\n",
       "     (0): Linear(in_features=320, out_features=2048, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): Dropout(p=0.3, inplace=False)\n",
       "     (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (7): Dropout(p=0.25, inplace=False)\n",
       "     (8): Linear(in_features=1024, out_features=6616, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:13.086603Z",
     "iopub.status.busy": "2025-12-05T08:22:13.086163Z",
     "iopub.status.idle": "2025-12-05T08:22:13.100884Z",
     "shell.execute_reply": "2025-12-05T08:22:13.100131Z",
     "shell.execute_reply.started": "2025-12-05T08:22:13.086586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       " \twith 250805 stored elements and shape (82404, 16858)>,\n",
       " 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       " \twith 157770 stored elements and shape (82404, 2651)>,\n",
       " 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       " \twith 128452 stored elements and shape (82404, 6616)>}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:13.101772Z",
     "iopub.status.busy": "2025-12-05T08:22:13.101550Z",
     "iopub.status.idle": "2025-12-05T08:22:25.262877Z",
     "shell.execute_reply": "2025-12-05T08:22:25.262150Z",
     "shell.execute_reply.started": "2025-12-05T08:22:13.101750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P loader ready: torch.Size([82404, 16858])\n",
      "C loader ready: torch.Size([82404, 2651])\n",
      "F loader ready: torch.Size([82404, 6616])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "\n",
    "loaders = {}\n",
    "\n",
    "for aspect in ['P', 'C', 'F']:\n",
    "    # Convert sparse CSR → dense numpy → float tensor\n",
    "    y_dense = y_trains[aspect].toarray().astype('float32')\n",
    "    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(X_train_tensor, y_tensor)\n",
    "\n",
    "    loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    print(aspect, \"loader ready:\", y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:25.263924Z",
     "iopub.status.busy": "2025-12-05T08:22:25.263645Z",
     "iopub.status.idle": "2025-12-05T08:22:25.269632Z",
     "shell.execute_reply": "2025-12-05T08:22:25.269005Z",
     "shell.execute_reply.started": "2025-12-05T08:22:25.263901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_one_model(model, loader, epochs=5, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)               # shape: (batch, num_labels)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs}   Loss = {total_loss/len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:22:25.270705Z",
     "iopub.status.busy": "2025-12-05T08:22:25.270407Z",
     "iopub.status.idle": "2025-12-05T08:24:45.179068Z",
     "shell.execute_reply": "2025-12-05T08:24:45.178319Z",
     "shell.execute_reply.started": "2025-12-05T08:22:25.270682Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training P ...\n",
      "Epoch 1/8   Loss = 0.0661\n",
      "Epoch 2/8   Loss = 0.0017\n",
      "Epoch 3/8   Loss = 0.0015\n",
      "Epoch 4/8   Loss = 0.0015\n",
      "Epoch 5/8   Loss = 0.0014\n",
      "Epoch 6/8   Loss = 0.0014\n",
      "Epoch 7/8   Loss = 0.0014\n",
      "Epoch 8/8   Loss = 0.0013\n",
      "\n",
      "Training C ...\n",
      "Epoch 1/8   Loss = 0.0679\n",
      "Epoch 2/8   Loss = 0.0038\n",
      "Epoch 3/8   Loss = 0.0036\n",
      "Epoch 4/8   Loss = 0.0034\n",
      "Epoch 5/8   Loss = 0.0033\n",
      "Epoch 6/8   Loss = 0.0032\n",
      "Epoch 7/8   Loss = 0.0032\n",
      "Epoch 8/8   Loss = 0.0031\n",
      "\n",
      "Training F ...\n",
      "Epoch 1/8   Loss = 0.0658\n",
      "Epoch 2/8   Loss = 0.0016\n",
      "Epoch 3/8   Loss = 0.0014\n",
      "Epoch 4/8   Loss = 0.0013\n",
      "Epoch 5/8   Loss = 0.0012\n",
      "Epoch 6/8   Loss = 0.0012\n",
      "Epoch 7/8   Loss = 0.0011\n",
      "Epoch 8/8   Loss = 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Train 3 models\n",
    "trained_models = dict()\n",
    "\n",
    "for aspect in ['P', 'C', 'F']:\n",
    "    print(\"\\nTraining\", aspect, \"...\")\n",
    "    trained_models[aspect] = train_one_model(\n",
    "        model=models[aspect],\n",
    "        loader=loaders[aspect],\n",
    "        epochs=8,\n",
    "        lr=1e-3,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:24:45.180411Z",
     "iopub.status.busy": "2025-12-05T08:24:45.180074Z",
     "iopub.status.idle": "2025-12-05T08:25:56.255000Z",
     "shell.execute_reply": "2025-12-05T08:25:56.254418Z",
     "shell.execute_reply.started": "2025-12-05T08:24:45.180388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Test Set: 100%|██████████| 45/45 [01:10<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5000  # avoid memory overflow\n",
    "submission_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting on Test Set\"):\n",
    "    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n",
    "\n",
    "    # Slice features\n",
    "    X_batch = X_test_norm[i : i + BATCH_SIZE]\n",
    "    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n",
    "\n",
    "    # For each ontology aspect (P, F, C)\n",
    "    for aspect, model in trained_models.items():\n",
    "        model.eval()\n",
    "\n",
    "        # Forward pass (logits → probabilities)\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_batch)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray (batch, num_labels)\n",
    "\n",
    "        mlb = mlb_dict[aspect]  # MultiLabelBinarizer for this aspect\n",
    "\n",
    "        # Loop over proteins in the batch\n",
    "        for j, entry_id in enumerate(batch_entry_ids):\n",
    "            prob_vec = probs[j]\n",
    "\n",
    "            # threshold at 0.02\n",
    "            candidate_indices = np.where(prob_vec > 0.02)[0]\n",
    "\n",
    "            for idx in candidate_indices:\n",
    "                submission_list.append(\n",
    "                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:25:56.255976Z",
     "iopub.status.busy": "2025-12-05T08:25:56.255736Z",
     "iopub.status.idle": "2025-12-05T08:26:08.653864Z",
     "shell.execute_reply": "2025-12-05T08:26:08.653294Z",
     "shell.execute_reply.started": "2025-12-05T08:25:56.255959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(\n",
    "    submission_list,\n",
    "    columns=['Protein Id', 'GO Term Id', 'Prediction']\n",
    ")\n",
    "\n",
    "submission_df = submission_df.sort_values(\n",
    "    by=['Protein Id', 'Prediction'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Limit 1500 predictions per protein\n",
    "final_submission_df = (\n",
    "    submission_df.groupby('Protein Id')\n",
    "    .head(1500)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T08:26:08.654815Z",
     "iopub.status.busy": "2025-12-05T08:26:08.654584Z",
     "iopub.status.idle": "2025-12-05T08:26:08.663591Z",
     "shell.execute_reply": "2025-12-05T08:26:08.662992Z",
     "shell.execute_reply.started": "2025-12-05T08:26:08.654788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.tsv' created successfully.\n",
      "Total predictions in final submission: 5,784,652\n",
      "Submission DataFrame Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein Id</th>\n",
       "      <th>GO Term Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005515</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0005886</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0004745</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0071768</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A017SE81</td>\n",
       "      <td>GO:0016020</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protein Id  GO Term Id  Prediction\n",
       "0  A0A017SE81  GO:0005515       0.158\n",
       "1  A0A017SE81  GO:0005886       0.101\n",
       "2  A0A017SE81  GO:0004745       0.084\n",
       "3  A0A017SE81  GO:0071768       0.069\n",
       "4  A0A017SE81  GO:0016020       0.053"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nSubmission file 'submission.tsv' created successfully.\")\n",
    "print(f\"Total predictions in final submission: {len(final_submission_df):,}\")\n",
    "print(\"Submission DataFrame Head:\")\n",
    "display(final_submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8917191,
     "sourceId": 13991224,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
