{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13991224,"sourceType":"datasetVersion","datasetId":8917191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### CAFA6-09-Model Improvement\n\nImprovement from CAFA6-08: use model ensemble (1D-CNN + MLP)\n\nFuture direction: Use model ensemble\n\nReferences:\n- https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n- (ESM-2 embeddings 320 features) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n- (MLP with ESM2) https://www.kaggle.com/code/jwang2025learning/cafa-6-function-prediction-using-prott5?scriptVersionId=282801093\n- (1D-CNN) https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn#----5.-GENERATING-PREDICTIONS----\n- (Idea) https://www.kaggle.com/code/nina2025/cafa-6-protein-function-prediction\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install biopython > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:47.958933Z","iopub.execute_input":"2025-12-06T15:22:47.959224Z","iopub.status.idle":"2025-12-06T15:22:53.396943Z","shell.execute_reply.started":"2025-12-06T15:22:47.959199Z","shell.execute_reply":"2025-12-06T15:22:53.395810Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Load CAFA6 files\n\n---","metadata":{}},{"cell_type":"code","source":"from Bio import SeqIO  # parse fasta file\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:53.398351Z","iopub.execute_input":"2025-12-06T15:22:53.398564Z","iopub.status.idle":"2025-12-06T15:22:53.713569Z","shell.execute_reply.started":"2025-12-06T15:22:53.398542Z","shell.execute_reply":"2025-12-06T15:22:53.712989Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# CAFA6 file paths\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTRAIN_TAXONOMY = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\nTEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:53.714194Z","iopub.execute_input":"2025-12-06T15:22:53.714537Z","iopub.status.idle":"2025-12-06T15:22:53.718274Z","shell.execute_reply.started":"2025-12-06T15:22:53.714511Z","shell.execute_reply":"2025-12-06T15:22:53.717546Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dict {entryId, seq}\ntrain_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\ntest_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:53.719711Z","iopub.execute_input":"2025-12-06T15:22:53.719890Z","iopub.status.idle":"2025-12-06T15:22:58.704577Z","shell.execute_reply.started":"2025-12-06T15:22:53.719874Z","shell.execute_reply":"2025-12-06T15:22:58.703950Z"}},"outputs":[{"name":"stdout","text":"Loaded 82404 train and 224309 test sequences\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Train dict:\", list(train_sequences.items())[0])\nprint(\"Test dict:\", list(test_sequences.items())[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:58.705283Z","iopub.execute_input":"2025-12-06T15:22:58.705525Z","iopub.status.idle":"2025-12-06T15:22:58.822924Z","shell.execute_reply.started":"2025-12-06T15:22:58.705507Z","shell.execute_reply":"2025-12-06T15:22:58.822165Z"}},"outputs":[{"name":"stdout","text":"Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\nTest dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_ids = [i.split('|')[1] for i in train_sequences.keys()]\ntest_ids = list(test_sequences.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:58.824037Z","iopub.execute_input":"2025-12-06T15:22:58.824648Z","iopub.status.idle":"2025-12-06T15:22:58.850027Z","shell.execute_reply.started":"2025-12-06T15:22:58.824621Z","shell.execute_reply":"2025-12-06T15:22:58.849289Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 2: Feature extraction\n  \n---","metadata":{}},{"cell_type":"code","source":"# Embeddings file paths\nESM_EMBEDDINGS = \"/kaggle/input/esm2-t6-8m-ur50d-cafa6\"\nTRAIN_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_train.npy\"\nTEST_EMBEDDINGS = ESM_EMBEDDINGS + \"/protein_embeddings_test.npy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:01.036768Z","iopub.execute_input":"2025-12-06T15:23:01.037294Z","iopub.status.idle":"2025-12-06T15:23:01.040656Z","shell.execute_reply.started":"2025-12-06T15:23:01.037271Z","shell.execute_reply":"2025-12-06T15:23:01.039847Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load embeddings\nX_train = np.load(TRAIN_EMBEDDINGS)\nX_test = np.load(TEST_EMBEDDINGS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:02.535173Z","iopub.execute_input":"2025-12-06T15:23:02.535942Z","iopub.status.idle":"2025-12-06T15:23:06.877379Z","shell.execute_reply.started":"2025-12-06T15:23:02.535914Z","shell.execute_reply":"2025-12-06T15:23:06.876542Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:06.878871Z","iopub.execute_input":"2025-12-06T15:23:06.879553Z","iopub.status.idle":"2025-12-06T15:23:06.883291Z","shell.execute_reply.started":"2025-12-06T15:23:06.879530Z","shell.execute_reply":"2025-12-06T15:23:06.882540Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (82404, 320)\nX_test shape: (224309, 320)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Normalization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_norm = scaler.fit_transform(X_train)\nX_test_norm = scaler.transform(X_test)\nprint(\"X_train_norm shape:\", X_train_norm.shape)\nprint(\"X_test_norm shape:\", X_test_norm.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:06.884161Z","iopub.execute_input":"2025-12-06T15:23:06.884391Z","iopub.status.idle":"2025-12-06T15:23:08.346929Z","shell.execute_reply.started":"2025-12-06T15:23:06.884371Z","shell.execute_reply":"2025-12-06T15:23:08.346172Z"}},"outputs":[{"name":"stdout","text":"X_train_norm shape: (82404, 320)\nX_test_norm shape: (224309, 320)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Step 3: Customized MLP\n\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:08.512692Z","iopub.execute_input":"2025-12-06T15:23:08.513025Z","iopub.status.idle":"2025-12-06T15:23:11.692958Z","shell.execute_reply.started":"2025-12-06T15:23:08.513005Z","shell.execute_reply":"2025-12-06T15:23:11.692183Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class MLPClassifier(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 2048),\n            nn.ReLU(),\n            nn.BatchNorm1d(2048),\n            nn.Dropout(0.3),\n\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.25),\n\n            nn.Linear(1024, output_dim)  # logits\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:11.694073Z","iopub.execute_input":"2025-12-06T15:23:11.694444Z","iopub.status.idle":"2025-12-06T15:23:11.699221Z","shell.execute_reply.started":"2025-12-06T15:23:11.694425Z","shell.execute_reply":"2025-12-06T15:23:11.698454Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Step 4: Customized CNN1D\n\nReference: https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn#----5.-GENERATING-PREDICTIONS----\n\n---","metadata":{}},{"cell_type":"code","source":"class CNN1D(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(CNN1D, self).__init__()\n        self.conv_block1 = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n        self.conv_block2 = nn.Sequential(\n            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n        \n        flattened_size = int(64 * input_dim / 4)\n        \n        self.fc_block = nn.Sequential(\n            nn.Linear(in_features=flattened_size, out_features=1024),\n            nn.ReLU(),\n            nn.Dropout(p=0.4), # Added Dropout to prevent overfitting\n            nn.Linear(in_features=1024, out_features=num_classes)\n        )\n\n    def forward(self, x):\n        # (batch_size, embed_size) -> (batch_size, 1, embed_size)\n        x = x.unsqueeze(1)  # (batch_size, 1, 320)\n        x = self.conv_block1(x)  # (batch_size, 32, 160)\n        x = self.conv_block2(x)  # (batch_size, 64, 80)\n        x = torch.flatten(x, 1)  # (batch_size, 64*80)\n        x = self.fc_block(x)  # (batch_size, num_classes)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:13.937622Z","iopub.execute_input":"2025-12-06T15:23:13.938424Z","iopub.status.idle":"2025-12-06T15:23:13.947369Z","shell.execute_reply.started":"2025-12-06T15:23:13.938392Z","shell.execute_reply":"2025-12-06T15:23:13.946709Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Step 5: Label encoding + MLP training\n\nWe divide all labels into three subsets (one for MF, one for BP, and one for CC).\n-  As a result, for each sequence, we will have 3 vectors as follows using multi-hot encoding (i.e. simply one-hot for multi-classification problems)\n    - For MF: [0 1 0 1 ... 0] of length num_unique(MF_GO_temrs)\n    - For BP: [0 1 0 1 ... 0] of length num_unique(BP_GO_temrs)\n    - For CC: [0 1 0 1 ... 0] of length num_unique(CC_GO_temrs)\n- Then we will train three separate models for each ontology. We use three models to predict for a single example in the test set, and gather the predictions.\n\n---","metadata":{}},{"cell_type":"code","source":"# Create three label sets\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\n\nmlp_y_trains = dict()\nmlp_mlb_dict = dict()\nmlp_models = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    # Filter the train_terms_df based on aspect\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n\n    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n\n    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n    # This ensures y_train is of shape (82404, ...)\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    # Multi-hot encoding, use sparse representation\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    mlp_y_trains[aspect] = y_train\n    \n    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t\\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n\n    # Save to dict\n    mlp_mlb_dict[aspect] = mlb\n    model = MLPClassifier(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n    mlp_models[aspect] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:18.931453Z","iopub.execute_input":"2025-12-06T15:23:18.931927Z","iopub.status.idle":"2025-12-06T15:23:22.763750Z","shell.execute_reply.started":"2025-12-06T15:23:18.931907Z","shell.execute_reply":"2025-12-06T15:23:22.762913Z"}},"outputs":[{"name":"stdout","text":"y_train shape for P ontology: (82404, 16858) \t\t Number of unique P terms: 16858\ny_train shape for C ontology: (82404, 2651) \t\t Number of unique C terms: 2651\ny_train shape for F ontology: (82404, 6616) \t\t Number of unique F terms: 6616\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"mlp_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:22.765270Z","iopub.execute_input":"2025-12-06T15:23:22.765671Z","iopub.status.idle":"2025-12-06T15:23:22.771304Z","shell.execute_reply.started":"2025-12-06T15:23:22.765652Z","shell.execute_reply":"2025-12-06T15:23:22.770605Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'P': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=320, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=16858, bias=True)\n   )\n ),\n 'C': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=320, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=2651, bias=True)\n   )\n ),\n 'F': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=320, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=6616, bias=True)\n   )\n )}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"mlp_y_trains","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:22.788516Z","iopub.execute_input":"2025-12-06T15:23:22.789395Z","iopub.status.idle":"2025-12-06T15:23:22.881236Z","shell.execute_reply.started":"2025-12-06T15:23:22.789280Z","shell.execute_reply":"2025-12-06T15:23:22.880572Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 250805 stored elements and shape (82404, 16858)>,\n 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 157770 stored elements and shape (82404, 2651)>,\n 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 128452 stored elements and shape (82404, 6616)>}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n\nloaders = {}\n\nfor aspect in ['P', 'C', 'F']:\n    # Convert sparse CSR → dense numpy → float tensor\n    y_dense = mlp_y_trains[aspect].toarray().astype('float32')\n    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n\n    dataset = TensorDataset(X_train_tensor, y_tensor)\n\n    loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n\n    print(aspect, \"loader ready:\", y_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:25.620518Z","iopub.execute_input":"2025-12-06T15:23:25.621018Z","iopub.status.idle":"2025-12-06T15:23:37.256771Z","shell.execute_reply.started":"2025-12-06T15:23:25.620992Z","shell.execute_reply":"2025-12-06T15:23:37.256017Z"}},"outputs":[{"name":"stdout","text":"P loader ready: torch.Size([82404, 16858])\nC loader ready: torch.Size([82404, 2651])\nF loader ready: torch.Size([82404, 6616])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Training\nimport torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\n\ndef train_one_model(model, loader, epochs=5, lr=1e-3, device='cuda'):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for ep in range(epochs):\n        total_loss = 0.0\n\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            logits = model(X_batch)               # shape: (batch, num_labels)\n            loss = criterion(logits, y_batch)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {ep+1}/{epochs}   Loss = {total_loss/len(loader):.4f}\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:37.257782Z","iopub.execute_input":"2025-12-06T15:23:37.257991Z","iopub.status.idle":"2025-12-06T15:23:37.265061Z","shell.execute_reply.started":"2025-12-06T15:23:37.257976Z","shell.execute_reply":"2025-12-06T15:23:37.264461Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Train 3 models\ntrained_mlp_models = dict()\n\nfor aspect in ['P', 'C', 'F']:\n    print(\"\\nTraining\", aspect, \"...\")\n    trained_mlp_models[aspect] = train_one_model(\n        model=mlp_models[aspect],\n        loader=loaders[aspect],\n        epochs=8,\n        lr=1e-3,\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:23:37.265652Z","iopub.execute_input":"2025-12-06T15:23:37.265816Z","iopub.status.idle":"2025-12-06T15:25:55.480204Z","shell.execute_reply.started":"2025-12-06T15:23:37.265801Z","shell.execute_reply":"2025-12-06T15:25:55.479382Z"}},"outputs":[{"name":"stdout","text":"\nTraining P ...\nEpoch 1/8   Loss = 0.0666\nEpoch 2/8   Loss = 0.0018\nEpoch 3/8   Loss = 0.0015\nEpoch 4/8   Loss = 0.0015\nEpoch 5/8   Loss = 0.0015\nEpoch 6/8   Loss = 0.0014\nEpoch 7/8   Loss = 0.0014\nEpoch 8/8   Loss = 0.0013\n\nTraining C ...\nEpoch 1/8   Loss = 0.0676\nEpoch 2/8   Loss = 0.0038\nEpoch 3/8   Loss = 0.0036\nEpoch 4/8   Loss = 0.0034\nEpoch 5/8   Loss = 0.0033\nEpoch 6/8   Loss = 0.0032\nEpoch 7/8   Loss = 0.0031\nEpoch 8/8   Loss = 0.0031\n\nTraining F ...\nEpoch 1/8   Loss = 0.0663\nEpoch 2/8   Loss = 0.0016\nEpoch 3/8   Loss = 0.0014\nEpoch 4/8   Loss = 0.0013\nEpoch 5/8   Loss = 0.0013\nEpoch 6/8   Loss = 0.0012\nEpoch 7/8   Loss = 0.0011\nEpoch 8/8   Loss = 0.0011\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 6: Label encoding + CNN1D model training\n\n---","metadata":{}},{"cell_type":"code","source":"# Create three label sets\ncnn_y_trains = dict()\ncnn_mlb_dict = dict()\ncnn_models = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    cnn_y_trains[aspect] = y_train\n\n    print(f\"{aspect} y_train shape:\", y_train.shape)\n\n    cnn_mlb_dict[aspect] = mlb\n    model = CNN1D(input_dim=X_train.shape[1], num_classes=y_train.shape[1])\n    cnn_models[aspect] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:25:58.861666Z","iopub.execute_input":"2025-12-06T15:25:58.862526Z","iopub.status.idle":"2025-12-06T15:26:02.826633Z","shell.execute_reply.started":"2025-12-06T15:25:58.862499Z","shell.execute_reply":"2025-12-06T15:26:02.825949Z"}},"outputs":[{"name":"stdout","text":"P y_train shape: (82404, 16858)\nC y_train shape: (82404, 2651)\nF y_train shape: (82404, 6616)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"cnn_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:26:02.827777Z","iopub.execute_input":"2025-12-06T15:26:02.828097Z","iopub.status.idle":"2025-12-06T15:26:02.833393Z","shell.execute_reply.started":"2025-12-06T15:26:02.828077Z","shell.execute_reply":"2025-12-06T15:26:02.832760Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'P': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=5120, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=16858, bias=True)\n   )\n ),\n 'C': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=5120, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=2651, bias=True)\n   )\n ),\n 'F': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=5120, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=6616, bias=True)\n   )\n )}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"cnn_y_trains","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:26:05.675902Z","iopub.execute_input":"2025-12-06T15:26:05.676694Z","iopub.status.idle":"2025-12-06T15:26:05.681235Z","shell.execute_reply.started":"2025-12-06T15:26:05.676666Z","shell.execute_reply":"2025-12-06T15:26:05.680654Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 250805 stored elements and shape (82404, 16858)>,\n 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 157770 stored elements and shape (82404, 2651)>,\n 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 128452 stored elements and shape (82404, 6616)>}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\ncnn_loaders = {}\n\nfor aspect in ['P', 'C', 'F']:\n    y_dense = cnn_y_trains[aspect].toarray().astype('float32')\n    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n\n    dataset = TensorDataset(X_train_tensor, y_tensor)\n\n    cnn_loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n\n    print(aspect, \"loader ready:\", y_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:26:07.637339Z","iopub.execute_input":"2025-12-06T15:26:07.637680Z","iopub.status.idle":"2025-12-06T15:26:19.245227Z","shell.execute_reply.started":"2025-12-06T15:26:07.637658Z","shell.execute_reply":"2025-12-06T15:26:19.244506Z"}},"outputs":[{"name":"stdout","text":"P loader ready: torch.Size([82404, 16858])\nC loader ready: torch.Size([82404, 2651])\nF loader ready: torch.Size([82404, 6616])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\n\ndef train_one_cnn(model, loader, epochs=8, lr=1e-3, device='cuda'):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    model.train()\n    for ep in range(epochs):\n        total_loss = 0.0\n        \n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            logits = model(X_batch)\n            loss = criterion(logits, y_batch)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg = total_loss / len(loader)\n        print(f\"Epoch {ep+1}/{epochs}   Loss = {avg:.4f}\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:26:19.246509Z","iopub.execute_input":"2025-12-06T15:26:19.246873Z","iopub.status.idle":"2025-12-06T15:26:19.253508Z","shell.execute_reply.started":"2025-12-06T15:26:19.246851Z","shell.execute_reply":"2025-12-06T15:26:19.252903Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trained_cnn_models = dict()\n\nfor aspect in ['P', 'C', 'F']:\n    print(\"\\nTraining CNN for\", aspect)\n    trained_cnn_models[aspect] = train_one_cnn(\n        model=cnn_models[aspect],\n        loader=cnn_loaders[aspect],\n        epochs=8,\n        lr=1e-3,\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:26:22.189833Z","iopub.execute_input":"2025-12-06T15:26:22.190305Z","iopub.status.idle":"2025-12-06T15:29:18.448609Z","shell.execute_reply.started":"2025-12-06T15:26:22.190282Z","shell.execute_reply":"2025-12-06T15:29:18.447915Z"}},"outputs":[{"name":"stdout","text":"\nTraining CNN for P\nEpoch 1/8   Loss = 0.0046\nEpoch 2/8   Loss = 0.0016\nEpoch 3/8   Loss = 0.0015\nEpoch 4/8   Loss = 0.0015\nEpoch 5/8   Loss = 0.0014\nEpoch 6/8   Loss = 0.0014\nEpoch 7/8   Loss = 0.0014\nEpoch 8/8   Loss = 0.0013\n\nTraining CNN for C\nEpoch 1/8   Loss = 0.0065\nEpoch 2/8   Loss = 0.0036\nEpoch 3/8   Loss = 0.0034\nEpoch 4/8   Loss = 0.0033\nEpoch 5/8   Loss = 0.0032\nEpoch 6/8   Loss = 0.0031\nEpoch 7/8   Loss = 0.0030\nEpoch 8/8   Loss = 0.0029\n\nTraining CNN for F\nEpoch 1/8   Loss = 0.0042\nEpoch 2/8   Loss = 0.0014\nEpoch 3/8   Loss = 0.0013\nEpoch 4/8   Loss = 0.0012\nEpoch 5/8   Loss = 0.0011\nEpoch 6/8   Loss = 0.0011\nEpoch 7/8   Loss = 0.0010\nEpoch 8/8   Loss = 0.0010\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Step 7: Inference and Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 5000  # avoid memory overflow\nmlp_submission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting with MLP\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    # Slice features\n    X_batch = X_test_norm[i : i + BATCH_SIZE]\n    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n\n    # For each ontology aspect (P, F, C)\n    for aspect, model in trained_mlp_models.items():\n        model.eval()\n\n        # Forward pass (logits → probabilities)\n        with torch.no_grad():\n            logits = model(X_batch)\n            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray (batch, num_labels)\n\n        mlb = mlp_mlb_dict[aspect]  # MultiLabelBinarizer for this aspect\n\n        # Loop over proteins in the batch\n        for j, entry_id in enumerate(batch_entry_ids):\n            prob_vec = probs[j]\n\n            # threshold at 0.02\n            candidate_indices = np.where(prob_vec > 0.02)[0]\n\n            for idx in candidate_indices:\n                mlp_submission_list.append(\n                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:29:20.567457Z","iopub.execute_input":"2025-12-06T15:29:20.568041Z","iopub.status.idle":"2025-12-06T15:30:32.913398Z","shell.execute_reply.started":"2025-12-06T15:29:20.568016Z","shell.execute_reply":"2025-12-06T15:30:32.912712Z"}},"outputs":[{"name":"stderr","text":"Predicting with MLP: 100%|██████████| 45/45 [01:12<00:00,  1.61s/it]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(f\"{len(mlp_submission_list):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:32:15.449517Z","iopub.execute_input":"2025-12-06T15:32:15.450174Z","iopub.status.idle":"2025-12-06T15:32:15.453742Z","shell.execute_reply.started":"2025-12-06T15:32:15.450152Z","shell.execute_reply":"2025-12-06T15:32:15.453161Z"}},"outputs":[{"name":"stdout","text":"5,786,687\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"BATCH_SIZE = 5000\ncnn_submission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting with CNN\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    # Slice features\n    X_batch = X_test_norm[i : i + BATCH_SIZE]\n    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n\n    # For each ontology aspect (P, F, C)\n    for aspect, model in trained_cnn_models.items():\n        model.eval()\n\n        with torch.no_grad():\n            logits = model(X_batch)                       # (batch, num_labels)\n            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray\n\n        mlb = cnn_mlb_dict[aspect]\n\n        # Loop over proteins in batch\n        for j, entry_id in enumerate(batch_entry_ids):\n            prob_vec = probs[j]\n\n            # threshold = 0.02 (same as your MLP)\n            candidate_indices = np.where(prob_vec > 0.02)[0]\n\n            for idx in candidate_indices:\n                cnn_submission_list.append(\n                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:33:20.358979Z","iopub.execute_input":"2025-12-06T15:33:20.359547Z","iopub.status.idle":"2025-12-06T15:34:31.263008Z","shell.execute_reply.started":"2025-12-06T15:33:20.359522Z","shell.execute_reply":"2025-12-06T15:34:31.262344Z"}},"outputs":[{"name":"stderr","text":"Predicting with CNN: 100%|██████████| 45/45 [01:10<00:00,  1.57s/it]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(f\"{len(cnn_submission_list):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:34:31.264272Z","iopub.execute_input":"2025-12-06T15:34:31.264610Z","iopub.status.idle":"2025-12-06T15:34:31.268582Z","shell.execute_reply.started":"2025-12-06T15:34:31.264591Z","shell.execute_reply":"2025-12-06T15:34:31.267790Z"}},"outputs":[{"name":"stdout","text":"5,198,930\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Ensemble the models\nmerged_dict = {}  # key: (entry_id, GO), value: list of scores\n\n# Add MLP predictions\nfor entry_id, go_term, score in tqdm(mlp_submission_list, desc=\"Merging MLP\"):\n    merged_dict.setdefault((entry_id, go_term), []).append(score)\n\n# Add CNN predictions\nfor entry_id, go_term, score in tqdm(cnn_submission_list, desc=\"Merging CNN\"):\n    merged_dict.setdefault((entry_id, go_term), []).append(score)\n\n# Final merged list\nsubmission_list = []\n\nfor (entry_id, go_term), scores in tqdm(merged_dict.items(), desc=\"Averaging\"):\n    final_score = round(sum(scores) / len(scores), 3)\n    submission_list.append((entry_id, go_term, final_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:35:51.618213Z","iopub.execute_input":"2025-12-06T15:35:51.618920Z","iopub.status.idle":"2025-12-06T15:37:06.769563Z","shell.execute_reply.started":"2025-12-06T15:35:51.618893Z","shell.execute_reply":"2025-12-06T15:37:06.768804Z"}},"outputs":[{"name":"stderr","text":"Merging MLP: 100%|██████████| 5786687/5786687 [00:12<00:00, 462712.66it/s] \nMerging CNN: 100%|██████████| 5198930/5198930 [00:05<00:00, 903228.67it/s] \nAveraging: 100%|██████████| 7796023/7796023 [00:56<00:00, 138840.31it/s]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"list(merged_dict.items())[2344]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:38:17.917784Z","iopub.execute_input":"2025-12-06T15:38:17.918526Z","iopub.status.idle":"2025-12-06T15:38:24.840507Z","shell.execute_reply.started":"2025-12-06T15:38:17.918500Z","shell.execute_reply":"2025-12-06T15:38:24.839730Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"(('O60284', 'GO:0006355'), [0.076, 0.059])"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"submission_df = pd.DataFrame(\n    submission_list,\n    columns=['Protein Id', 'GO Term Id', 'Prediction']\n)\n\nsubmission_df = submission_df.sort_values(\n    by=['Protein Id', 'Prediction'],\n    ascending=[True, False]\n)\n\n# Limit 1500 predictions per protein\nfinal_submission_df = (\n    submission_df.groupby('Protein Id')\n    .head(1500)\n    .reset_index(drop=True)\n)\n\nfinal_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:38:37.803761Z","iopub.execute_input":"2025-12-06T15:38:37.804514Z","iopub.status.idle":"2025-12-06T15:38:53.919173Z","shell.execute_reply.started":"2025-12-06T15:38:37.804487Z","shell.execute_reply":"2025-12-06T15:38:53.918571Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"print(\"\\nSubmission file 'submission.tsv' created successfully.\")\nprint(f\"Total predictions in final submission: {len(final_submission_df):,}\")\nprint(\"Submission DataFrame Head:\")\ndisplay(final_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:38:53.920249Z","iopub.execute_input":"2025-12-06T15:38:53.920597Z","iopub.status.idle":"2025-12-06T15:38:53.937046Z","shell.execute_reply.started":"2025-12-06T15:38:53.920572Z","shell.execute_reply":"2025-12-06T15:38:53.936539Z"}},"outputs":[{"name":"stdout","text":"\nSubmission file 'submission.tsv' created successfully.\nTotal predictions in final submission: 7,796,023\nSubmission DataFrame Head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Protein Id  GO Term Id  Prediction\n0  A0A017SE81  GO:0005515       0.213\n1  A0A017SE81  GO:0004745       0.149\n2  A0A017SE81  GO:0004318       0.138\n3  A0A017SE81  GO:0003824       0.127\n4  A0A017SE81  GO:0016616       0.113","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Protein Id</th>\n      <th>GO Term Id</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005515</td>\n      <td>0.213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A017SE81</td>\n      <td>GO:0004745</td>\n      <td>0.149</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A017SE81</td>\n      <td>GO:0004318</td>\n      <td>0.138</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A017SE81</td>\n      <td>GO:0003824</td>\n      <td>0.127</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A017SE81</td>\n      <td>GO:0016616</td>\n      <td>0.113</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":45}]}