{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13991224,"sourceType":"datasetVersion","datasetId":8917191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### CAFA6-09-Model Improvement\n\nImprovement from CAFA6-08: use model ensemble (1D-CNN + MLP)\n\nFuture direction: Use model ensemble\n\nReferences:\n- https://www.kaggle.com/code/analyticaobscura/cafa-6-decoding-protein-mysteries\n- (ESM-2 embeddings 320 features) https://www.kaggle.com/code/dalloliogm/compute-protein-embeddings-with-esm2-esm-c/notebook\n- (MLP with ESM2) https://www.kaggle.com/code/jwang2025learning/cafa-6-function-prediction-using-prott5?scriptVersionId=282801093\n- (1D-CNN) https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn#----5.-GENERATING-PREDICTIONS----\n- (Idea) https://www.kaggle.com/code/nina2025/cafa-6-protein-function-prediction\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install biopython > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:10:52.406934Z","iopub.execute_input":"2025-12-07T02:10:52.407215Z","iopub.status.idle":"2025-12-07T02:10:58.343383Z","shell.execute_reply.started":"2025-12-07T02:10:52.407192Z","shell.execute_reply":"2025-12-07T02:10:58.342400Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Load CAFA6 files\n\n---","metadata":{}},{"cell_type":"code","source":"from Bio import SeqIO  # parse fasta file\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:10:58.344778Z","iopub.execute_input":"2025-12-07T02:10:58.345005Z","iopub.status.idle":"2025-12-07T02:10:58.721504Z","shell.execute_reply.started":"2025-12-07T02:10:58.344982Z","shell.execute_reply":"2025-12-07T02:10:58.720928Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# CAFA6 file paths\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTRAIN_TAXONOMY = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\nTEST_SEQ = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:10:58.722121Z","iopub.execute_input":"2025-12-07T02:10:58.722481Z","iopub.status.idle":"2025-12-07T02:10:58.726207Z","shell.execute_reply.started":"2025-12-07T02:10:58.722462Z","shell.execute_reply":"2025-12-07T02:10:58.725539Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dict {entryId, seq}\ntrain_sequences = {rec.id: str(rec.seq) for rec in SeqIO.parse(TRAIN_SEQ, 'fasta')}\ntest_sequences  = {rec.id: str(rec.seq) for rec in SeqIO.parse(TEST_SEQ,  'fasta')}\n\nprint(f'Loaded {len(train_sequences)} train and {len(test_sequences)} test sequences')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:10:58.727916Z","iopub.execute_input":"2025-12-07T02:10:58.728095Z","iopub.status.idle":"2025-12-07T02:11:01.889047Z","shell.execute_reply.started":"2025-12-07T02:10:58.728080Z","shell.execute_reply":"2025-12-07T02:11:01.888271Z"}},"outputs":[{"name":"stdout","text":"Loaded 82404 train and 224309 test sequences\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Train dict:\", list(train_sequences.items())[0])\nprint(\"Test dict:\", list(test_sequences.items())[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:01.889746Z","iopub.execute_input":"2025-12-07T02:11:01.889941Z","iopub.status.idle":"2025-12-07T02:11:02.012704Z","shell.execute_reply.started":"2025-12-07T02:11:01.889926Z","shell.execute_reply":"2025-12-07T02:11:02.011941Z"}},"outputs":[{"name":"stdout","text":"Train dict: ('sp|A0A0C5B5G6|MOTSC_HUMAN', 'MRWQEMGYIFYPRKLR')\nTest dict: ('A0A0C5B5G6', 'MRWQEMGYIFYPRKLR')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_ids = [i.split('|')[1] for i in train_sequences.keys()]\ntest_ids = list(test_sequences.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:02.013462Z","iopub.execute_input":"2025-12-07T02:11:02.013730Z","iopub.status.idle":"2025-12-07T02:11:02.044331Z","shell.execute_reply.started":"2025-12-07T02:11:02.013711Z","shell.execute_reply":"2025-12-07T02:11:02.043566Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 2: Feature extraction\n  \n---","metadata":{}},{"cell_type":"code","source":"# Embeddings file paths\nT5_EMBEDDINGS = \"/kaggle/input/prott5-embeddings-cafa6\"\nTRAIN_EMBEDDINGS = T5_EMBEDDINGS + \"/train_embeds.npy\"\nTEST_EMBEDDINGS = T5_EMBEDDINGS + \"/test_embeds.npy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:18.049669Z","iopub.execute_input":"2025-12-07T02:11:18.050397Z","iopub.status.idle":"2025-12-07T02:11:18.053882Z","shell.execute_reply.started":"2025-12-07T02:11:18.050367Z","shell.execute_reply":"2025-12-07T02:11:18.053276Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load embeddings\nX_train = np.load(TRAIN_EMBEDDINGS)\nX_test = np.load(TEST_EMBEDDINGS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:18.981520Z","iopub.execute_input":"2025-12-07T02:11:18.982193Z","iopub.status.idle":"2025-12-07T02:11:22.804965Z","shell.execute_reply.started":"2025-12-07T02:11:18.982162Z","shell.execute_reply":"2025-12-07T02:11:22.804461Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:22.806173Z","iopub.execute_input":"2025-12-07T02:11:22.806425Z","iopub.status.idle":"2025-12-07T02:11:22.810320Z","shell.execute_reply.started":"2025-12-07T02:11:22.806404Z","shell.execute_reply":"2025-12-07T02:11:22.809586Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (82404, 1024)\nX_test shape: (224309, 1024)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Normalization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_norm = scaler.fit_transform(X_train)\nX_test_norm = scaler.transform(X_test)\nprint(\"X_train_norm shape:\", X_train_norm.shape)\nprint(\"X_test_norm shape:\", X_test_norm.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:27.608044Z","iopub.execute_input":"2025-12-07T02:11:27.608758Z","iopub.status.idle":"2025-12-07T02:11:30.589491Z","shell.execute_reply.started":"2025-12-07T02:11:27.608715Z","shell.execute_reply":"2025-12-07T02:11:30.588752Z"}},"outputs":[{"name":"stdout","text":"X_train_norm shape: (82404, 1024)\nX_test_norm shape: (224309, 1024)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Step 3: Customized MLP\n\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:33.698948Z","iopub.execute_input":"2025-12-07T02:11:33.699373Z","iopub.status.idle":"2025-12-07T02:11:37.466999Z","shell.execute_reply.started":"2025-12-07T02:11:33.699351Z","shell.execute_reply":"2025-12-07T02:11:37.466196Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class MLPClassifier(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 2048),\n            nn.ReLU(),\n            nn.BatchNorm1d(2048),\n            nn.Dropout(0.3),\n\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.25),\n\n            nn.Linear(1024, output_dim)  # logits\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:37.468340Z","iopub.execute_input":"2025-12-07T02:11:37.468764Z","iopub.status.idle":"2025-12-07T02:11:37.473600Z","shell.execute_reply.started":"2025-12-07T02:11:37.468744Z","shell.execute_reply":"2025-12-07T02:11:37.472876Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Step 4: Customized CNN1D\n\nReference: https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn#----5.-GENERATING-PREDICTIONS----\n\n---","metadata":{}},{"cell_type":"code","source":"class CNN1D(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(CNN1D, self).__init__()\n        self.conv_block1 = nn.Sequential(\n            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n        self.conv_block2 = nn.Sequential(\n            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2)\n        )\n        \n        flattened_size = int(64 * input_dim / 4)\n        \n        self.fc_block = nn.Sequential(\n            nn.Linear(in_features=flattened_size, out_features=1024),\n            nn.ReLU(),\n            nn.Dropout(p=0.4), # Added Dropout to prevent overfitting\n            nn.Linear(in_features=1024, out_features=num_classes)\n        )\n\n    def forward(self, x):\n        # (batch_size, embed_size) -> (batch_size, 1, embed_size)\n        x = x.unsqueeze(1)  # (batch_size, 1, 320)\n        x = self.conv_block1(x)  # (batch_size, 32, 160)\n        x = self.conv_block2(x)  # (batch_size, 64, 80)\n        x = torch.flatten(x, 1)  # (batch_size, 64*80)\n        x = self.fc_block(x)  # (batch_size, num_classes)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:37.474319Z","iopub.execute_input":"2025-12-07T02:11:37.474561Z","iopub.status.idle":"2025-12-07T02:11:37.502112Z","shell.execute_reply.started":"2025-12-07T02:11:37.474538Z","shell.execute_reply":"2025-12-07T02:11:37.501458Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Step 5: Label encoding + MLP training\n\nWe divide all labels into three subsets (one for MF, one for BP, and one for CC).\n-  As a result, for each sequence, we will have 3 vectors as follows using multi-hot encoding (i.e. simply one-hot for multi-classification problems)\n    - For MF: [0 1 0 1 ... 0] of length num_unique(MF_GO_temrs)\n    - For BP: [0 1 0 1 ... 0] of length num_unique(BP_GO_temrs)\n    - For CC: [0 1 0 1 ... 0] of length num_unique(CC_GO_temrs)\n- Then we will train three separate models for each ontology. We use three models to predict for a single example in the test set, and gather the predictions.\n\n---","metadata":{}},{"cell_type":"code","source":"# Create three label sets\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\n\nmlp_y_trains = dict()\nmlp_mlb_dict = dict()\nmlp_models = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    # Filter the train_terms_df based on aspect\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n\n    # Group the dataFrame based on the EntryID, turn all the GO terms to a list, finally turns this dataFrame to a dict {entryID: terms}\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n\n    # Create a list of labels for this aspect, if an entryID doesn't exist in this aspect, give it a []\n    # This ensures y_train is of shape (82404, ...)\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    # Multi-hot encoding, use sparse representation\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    mlp_y_trains[aspect] = y_train\n    \n    print(f\"y_train shape for {aspect} ontology: {y_train.shape} \\t\\t Number of unique {aspect} terms: {y_train.shape[1]}\")\n\n    # Save to dict\n    mlp_mlb_dict[aspect] = mlb\n    model = MLPClassifier(input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n    mlp_models[aspect] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:40.359100Z","iopub.execute_input":"2025-12-07T02:11:40.359861Z","iopub.status.idle":"2025-12-07T02:11:44.377945Z","shell.execute_reply.started":"2025-12-07T02:11:40.359834Z","shell.execute_reply":"2025-12-07T02:11:44.377285Z"}},"outputs":[{"name":"stdout","text":"y_train shape for P ontology: (82404, 16858) \t\t Number of unique P terms: 16858\ny_train shape for C ontology: (82404, 2651) \t\t Number of unique C terms: 2651\ny_train shape for F ontology: (82404, 6616) \t\t Number of unique F terms: 6616\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"mlp_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:46.227892Z","iopub.execute_input":"2025-12-07T02:11:46.228548Z","iopub.status.idle":"2025-12-07T02:11:46.234015Z","shell.execute_reply.started":"2025-12-07T02:11:46.228518Z","shell.execute_reply":"2025-12-07T02:11:46.233448Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'P': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=1024, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=16858, bias=True)\n   )\n ),\n 'C': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=1024, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=2651, bias=True)\n   )\n ),\n 'F': MLPClassifier(\n   (net): Sequential(\n     (0): Linear(in_features=1024, out_features=2048, bias=True)\n     (1): ReLU()\n     (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (3): Dropout(p=0.3, inplace=False)\n     (4): Linear(in_features=2048, out_features=1024, bias=True)\n     (5): ReLU()\n     (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (7): Dropout(p=0.25, inplace=False)\n     (8): Linear(in_features=1024, out_features=6616, bias=True)\n   )\n )}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"mlp_y_trains","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:52.199653Z","iopub.execute_input":"2025-12-07T02:11:52.200342Z","iopub.status.idle":"2025-12-07T02:11:52.204899Z","shell.execute_reply.started":"2025-12-07T02:11:52.200317Z","shell.execute_reply":"2025-12-07T02:11:52.204262Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 250805 stored elements and shape (82404, 16858)>,\n 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 157770 stored elements and shape (82404, 2651)>,\n 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 128452 stored elements and shape (82404, 6616)>}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# DataLoader\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n\nloaders = {}\n\nfor aspect in ['P', 'C', 'F']:\n    # Convert sparse CSR → dense numpy → float tensor\n    y_dense = mlp_y_trains[aspect].toarray().astype('float32')\n    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n\n    dataset = TensorDataset(X_train_tensor, y_tensor)\n\n    loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n\n    print(aspect, \"loader ready:\", y_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:11:52.683089Z","iopub.execute_input":"2025-12-07T02:11:52.683813Z","iopub.status.idle":"2025-12-07T02:12:04.909638Z","shell.execute_reply.started":"2025-12-07T02:11:52.683787Z","shell.execute_reply":"2025-12-07T02:12:04.908796Z"}},"outputs":[{"name":"stdout","text":"P loader ready: torch.Size([82404, 16858])\nC loader ready: torch.Size([82404, 2651])\nF loader ready: torch.Size([82404, 6616])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Training\nimport torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\n\ndef train_one_model(model, loader, epochs=5, lr=1e-3, device='cuda'):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for ep in range(epochs):\n        total_loss = 0.0\n\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            logits = model(X_batch)               # shape: (batch, num_labels)\n            loss = criterion(logits, y_batch)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {ep+1}/{epochs}   Loss = {total_loss/len(loader):.4f}\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:12:04.910914Z","iopub.execute_input":"2025-12-07T02:12:04.911285Z","iopub.status.idle":"2025-12-07T02:12:04.917087Z","shell.execute_reply.started":"2025-12-07T02:12:04.911249Z","shell.execute_reply":"2025-12-07T02:12:04.916451Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Train 3 models\ntrained_mlp_models = dict()\n\nfor aspect in ['P', 'C', 'F']:\n    print(\"\\nTraining\", aspect, \"...\")\n    trained_mlp_models[aspect] = train_one_model(\n        model=mlp_models[aspect],\n        loader=loaders[aspect],\n        epochs=8,\n        lr=1e-3,\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:12:05.786467Z","iopub.execute_input":"2025-12-07T02:12:05.786773Z","iopub.status.idle":"2025-12-07T02:14:34.362748Z","shell.execute_reply.started":"2025-12-07T02:12:05.786744Z","shell.execute_reply":"2025-12-07T02:14:34.361793Z"}},"outputs":[{"name":"stdout","text":"\nTraining P ...\nEpoch 1/8   Loss = 0.0434\nEpoch 2/8   Loss = 0.0028\nEpoch 3/8   Loss = 0.0023\nEpoch 4/8   Loss = 0.0019\nEpoch 5/8   Loss = 0.0017\nEpoch 6/8   Loss = 0.0017\nEpoch 7/8   Loss = 0.0016\nEpoch 8/8   Loss = 0.0016\n\nTraining C ...\nEpoch 1/8   Loss = 0.0424\nEpoch 2/8   Loss = 0.0074\nEpoch 3/8   Loss = 0.0048\nEpoch 4/8   Loss = 0.0050\nEpoch 5/8   Loss = 0.0042\nEpoch 6/8   Loss = 0.0041\nEpoch 7/8   Loss = 0.0041\nEpoch 8/8   Loss = 0.0041\n\nTraining F ...\nEpoch 1/8   Loss = 0.0399\nEpoch 2/8   Loss = 0.0047\nEpoch 3/8   Loss = 0.0025\nEpoch 4/8   Loss = 0.0028\nEpoch 5/8   Loss = 0.0025\nEpoch 6/8   Loss = 0.0017\nEpoch 7/8   Loss = 0.0017\nEpoch 8/8   Loss = 0.0016\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Step 6: Label encoding + CNN1D model training\n\n---","metadata":{}},{"cell_type":"code","source":"# Create three label sets\ncnn_y_trains = dict()\ncnn_mlb_dict = dict()\ncnn_models = dict()\n\ntrain_terms_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\n\nfor aspect in ['P', 'C', 'F']:\n    ont_terms_df = train_terms_df[train_terms_df['aspect'] == aspect]\n    protein_terms = ont_terms_df.groupby('EntryID')['term'].apply(list).to_dict()\n    labels = [protein_terms.get(entry_id, []) for entry_id in train_ids]\n\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform(labels)\n    cnn_y_trains[aspect] = y_train\n\n    print(f\"{aspect} y_train shape:\", y_train.shape)\n\n    cnn_mlb_dict[aspect] = mlb\n    model = CNN1D(input_dim=X_train.shape[1], num_classes=y_train.shape[1])\n    cnn_models[aspect] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:37.642364Z","iopub.execute_input":"2025-12-07T02:14:37.643319Z","iopub.status.idle":"2025-12-07T02:14:42.435541Z","shell.execute_reply.started":"2025-12-07T02:14:37.643290Z","shell.execute_reply":"2025-12-07T02:14:42.434890Z"}},"outputs":[{"name":"stdout","text":"P y_train shape: (82404, 16858)\nC y_train shape: (82404, 2651)\nF y_train shape: (82404, 6616)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"cnn_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:42.436659Z","iopub.execute_input":"2025-12-07T02:14:42.436882Z","iopub.status.idle":"2025-12-07T02:14:42.442836Z","shell.execute_reply.started":"2025-12-07T02:14:42.436865Z","shell.execute_reply":"2025-12-07T02:14:42.442006Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'P': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=16384, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=16858, bias=True)\n   )\n ),\n 'C': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=16384, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=2651, bias=True)\n   )\n ),\n 'F': CNN1D(\n   (conv_block1): Sequential(\n     (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (conv_block2): Sequential(\n     (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n     (2): ReLU()\n     (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n   )\n   (fc_block): Sequential(\n     (0): Linear(in_features=16384, out_features=1024, bias=True)\n     (1): ReLU()\n     (2): Dropout(p=0.4, inplace=False)\n     (3): Linear(in_features=1024, out_features=6616, bias=True)\n   )\n )}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"cnn_y_trains","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:42.443692Z","iopub.execute_input":"2025-12-07T02:14:42.444257Z","iopub.status.idle":"2025-12-07T02:14:42.482635Z","shell.execute_reply.started":"2025-12-07T02:14:42.444233Z","shell.execute_reply":"2025-12-07T02:14:42.481955Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'P': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 250805 stored elements and shape (82404, 16858)>,\n 'C': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 157770 stored elements and shape (82404, 2651)>,\n 'F': <Compressed Sparse Row sparse matrix of dtype 'int64'\n \twith 128452 stored elements and shape (82404, 6616)>}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\ncnn_loaders = {}\n\nfor aspect in ['P', 'C', 'F']:\n    y_dense = cnn_y_trains[aspect].toarray().astype('float32')\n    y_tensor = torch.tensor(y_dense, dtype=torch.float32)\n\n    dataset = TensorDataset(X_train_tensor, y_tensor)\n\n    cnn_loaders[aspect] = DataLoader(dataset, batch_size=128, shuffle=True)\n\n    print(aspect, \"loader ready:\", y_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:43.968439Z","iopub.execute_input":"2025-12-07T02:14:43.969275Z","iopub.status.idle":"2025-12-07T02:14:56.380521Z","shell.execute_reply.started":"2025-12-07T02:14:43.969240Z","shell.execute_reply":"2025-12-07T02:14:56.379756Z"}},"outputs":[{"name":"stdout","text":"P loader ready: torch.Size([82404, 16858])\nC loader ready: torch.Size([82404, 2651])\nF loader ready: torch.Size([82404, 6616])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\n\ndef train_one_cnn(model, loader, epochs=8, lr=1e-3, device='cuda'):\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    model.train()\n    for ep in range(epochs):\n        total_loss = 0.0\n        \n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            logits = model(X_batch)\n            loss = criterion(logits, y_batch)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg = total_loss / len(loader)\n        print(f\"Epoch {ep+1}/{epochs}   Loss = {avg:.4f}\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:56.381909Z","iopub.execute_input":"2025-12-07T02:14:56.382219Z","iopub.status.idle":"2025-12-07T02:14:56.388536Z","shell.execute_reply.started":"2025-12-07T02:14:56.382194Z","shell.execute_reply":"2025-12-07T02:14:56.387679Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"trained_cnn_models = dict()\n\nfor aspect in ['P', 'C', 'F']:\n    print(\"\\nTraining CNN for\", aspect)\n    trained_cnn_models[aspect] = train_one_cnn(\n        model=cnn_models[aspect],\n        loader=cnn_loaders[aspect],\n        epochs=8,\n        lr=1e-3,\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:56.389309Z","iopub.execute_input":"2025-12-07T02:14:56.389562Z","iopub.status.idle":"2025-12-07T02:20:11.935781Z","shell.execute_reply.started":"2025-12-07T02:14:56.389541Z","shell.execute_reply":"2025-12-07T02:20:11.934936Z"}},"outputs":[{"name":"stdout","text":"\nTraining CNN for P\nEpoch 1/8   Loss = 0.0069\nEpoch 2/8   Loss = 0.0018\nEpoch 3/8   Loss = 0.0017\nEpoch 4/8   Loss = 0.0017\nEpoch 5/8   Loss = 0.0017\nEpoch 6/8   Loss = 0.0017\nEpoch 7/8   Loss = 0.0017\nEpoch 8/8   Loss = 0.0018\n\nTraining CNN for C\nEpoch 1/8   Loss = 0.0082\nEpoch 2/8   Loss = 0.0043\nEpoch 3/8   Loss = 0.0043\nEpoch 4/8   Loss = 0.0043\nEpoch 5/8   Loss = 0.0044\nEpoch 6/8   Loss = 0.0044\nEpoch 7/8   Loss = 0.0044\nEpoch 8/8   Loss = 0.0044\n\nTraining CNN for F\nEpoch 1/8   Loss = 0.0061\nEpoch 2/8   Loss = 0.0016\nEpoch 3/8   Loss = 0.0016\nEpoch 4/8   Loss = 0.0016\nEpoch 5/8   Loss = 0.0016\nEpoch 6/8   Loss = 0.0016\nEpoch 7/8   Loss = 0.0017\nEpoch 8/8   Loss = 0.0017\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Step 7: Inference and Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 5000  # avoid memory overflow\nmlp_submission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting with MLP\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    # Slice features\n    X_batch = X_test_norm[i : i + BATCH_SIZE]\n    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n\n    # For each ontology aspect (P, F, C)\n    for aspect, model in trained_mlp_models.items():\n        model.eval()\n\n        # Forward pass (logits → probabilities)\n        with torch.no_grad():\n            logits = model(X_batch)\n            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray (batch, num_labels)\n\n        mlb = mlp_mlb_dict[aspect]  # MultiLabelBinarizer for this aspect\n\n        # Loop over proteins in the batch\n        for j, entry_id in enumerate(batch_entry_ids):\n            prob_vec = probs[j]\n\n            # threshold at 0.02\n            candidate_indices = np.where(prob_vec > 0.02)[0]\n\n            for idx in candidate_indices:\n                mlp_submission_list.append(\n                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:20:51.218122Z","iopub.execute_input":"2025-12-07T02:20:51.218398Z","iopub.status.idle":"2025-12-07T02:21:53.660305Z","shell.execute_reply.started":"2025-12-07T02:20:51.218380Z","shell.execute_reply":"2025-12-07T02:21:53.659577Z"}},"outputs":[{"name":"stderr","text":"Predicting with MLP: 100%|██████████| 45/45 [01:02<00:00,  1.39s/it]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(f\"{len(mlp_submission_list):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:21:53.661611Z","iopub.execute_input":"2025-12-07T02:21:53.661851Z","iopub.status.idle":"2025-12-07T02:21:53.666068Z","shell.execute_reply.started":"2025-12-07T02:21:53.661831Z","shell.execute_reply":"2025-12-07T02:21:53.665373Z"}},"outputs":[{"name":"stdout","text":"4,037,551\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"BATCH_SIZE = 5000\ncnn_submission_list = []\n\nfor i in tqdm(range(0, len(test_ids), BATCH_SIZE), desc=\"Predicting with CNN\"):\n    batch_entry_ids = test_ids[i : i + BATCH_SIZE]\n\n    # Slice features\n    X_batch = X_test_norm[i : i + BATCH_SIZE]\n    X_batch = torch.tensor(X_batch, dtype=torch.float32, device=device)\n\n    # For each ontology aspect (P, F, C)\n    for aspect, model in trained_cnn_models.items():\n        model.eval()\n\n        with torch.no_grad():\n            logits = model(X_batch)                       # (batch, num_labels)\n            probs = torch.sigmoid(logits).cpu().numpy()  # ndarray\n\n        mlb = cnn_mlb_dict[aspect]\n\n        # Loop over proteins in batch\n        for j, entry_id in enumerate(batch_entry_ids):\n            prob_vec = probs[j]\n\n            # threshold = 0.02 (same as your MLP)\n            candidate_indices = np.where(prob_vec > 0.02)[0]\n\n            for idx in candidate_indices:\n                cnn_submission_list.append(\n                    (entry_id, mlb.classes_[idx], round(prob_vec[idx], 3))\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:21:53.666809Z","iopub.execute_input":"2025-12-07T02:21:53.667030Z","iopub.status.idle":"2025-12-07T02:22:46.355986Z","shell.execute_reply.started":"2025-12-07T02:21:53.667003Z","shell.execute_reply":"2025-12-07T02:22:46.355126Z"}},"outputs":[{"name":"stderr","text":"Predicting with CNN: 100%|██████████| 45/45 [00:52<00:00,  1.17s/it]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(f\"{len(cnn_submission_list):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:22:46.357839Z","iopub.execute_input":"2025-12-07T02:22:46.358083Z","iopub.status.idle":"2025-12-07T02:22:46.362606Z","shell.execute_reply.started":"2025-12-07T02:22:46.358063Z","shell.execute_reply":"2025-12-07T02:22:46.361911Z"}},"outputs":[{"name":"stdout","text":"897,246\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Ensemble the models\nmerged_dict = {}  # key: (entry_id, GO), value: list of scores\n\n# Add MLP predictions\nfor entry_id, go_term, score in tqdm(mlp_submission_list, desc=\"Merging MLP\"):\n    merged_dict.setdefault((entry_id, go_term), []).append(score)\n\n# Add CNN predictions\nfor entry_id, go_term, score in tqdm(cnn_submission_list, desc=\"Merging CNN\"):\n    merged_dict.setdefault((entry_id, go_term), []).append(score)\n\n# Final merged list\nsubmission_list = []\n\nfor (entry_id, go_term), scores in tqdm(merged_dict.items(), desc=\"Averaging\"):\n    final_score = round(sum(scores) / len(scores), 3)\n    submission_list.append((entry_id, go_term, final_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:22:52.550523Z","iopub.execute_input":"2025-12-07T02:22:52.551275Z","iopub.status.idle":"2025-12-07T02:23:31.252065Z","shell.execute_reply.started":"2025-12-07T02:22:52.551252Z","shell.execute_reply":"2025-12-07T02:23:31.251344Z"}},"outputs":[{"name":"stderr","text":"Merging MLP: 100%|██████████| 4037551/4037551 [00:07<00:00, 531841.30it/s]\nMerging CNN: 100%|██████████| 897246/897246 [00:00<00:00, 1133584.48it/s]\nAveraging: 100%|██████████| 4037560/4037560 [00:30<00:00, 133227.40it/s]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"list(merged_dict.items())[2344]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:38:17.917784Z","iopub.execute_input":"2025-12-06T15:38:17.918526Z","iopub.status.idle":"2025-12-06T15:38:24.840507Z","shell.execute_reply.started":"2025-12-06T15:38:17.9185Z","shell.execute_reply":"2025-12-06T15:38:24.83973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.DataFrame(\n    submission_list,\n    columns=['Protein Id', 'GO Term Id', 'Prediction']\n)\n\nsubmission_df = submission_df.sort_values(\n    by=['Protein Id', 'Prediction'],\n    ascending=[True, False]\n)\n\n# Limit 1500 predictions per protein\nfinal_submission_df = (\n    submission_df.groupby('Protein Id')\n    .head(1500)\n    .reset_index(drop=True)\n)\n\nfinal_submission_df.to_csv('submission.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:23:31.253439Z","iopub.execute_input":"2025-12-07T02:23:31.253661Z","iopub.status.idle":"2025-12-07T02:23:39.511280Z","shell.execute_reply.started":"2025-12-07T02:23:31.253645Z","shell.execute_reply":"2025-12-07T02:23:39.510461Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"print(\"\\nSubmission file 'submission.tsv' created successfully.\")\nprint(f\"Total predictions in final submission: {len(final_submission_df):,}\")\nprint(\"Submission DataFrame Head:\")\ndisplay(final_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:23:39.512067Z","iopub.execute_input":"2025-12-07T02:23:39.512347Z","iopub.status.idle":"2025-12-07T02:23:39.531343Z","shell.execute_reply.started":"2025-12-07T02:23:39.512325Z","shell.execute_reply":"2025-12-07T02:23:39.530648Z"}},"outputs":[{"name":"stdout","text":"\nSubmission file 'submission.tsv' created successfully.\nTotal predictions in final submission: 4,037,560\nSubmission DataFrame Head:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Protein Id  GO Term Id  Prediction\n0  A0A017SE81  GO:0005515       0.731\n1  A0A017SE81  GO:0005739       0.454\n2  A0A017SE81  GO:0005829       0.450\n3  A0A017SE81  GO:0005634       0.267\n4  A0A017SE81  GO:0005737       0.122","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Protein Id</th>\n      <th>GO Term Id</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005515</td>\n      <td>0.731</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005739</td>\n      <td>0.454</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005829</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005634</td>\n      <td>0.267</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A017SE81</td>\n      <td>GO:0005737</td>\n      <td>0.122</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33}]}